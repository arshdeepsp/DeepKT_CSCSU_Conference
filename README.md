# Knowledge Tracing: LSTM vs. Transformer Models  

## ðŸ“Œ Overview  
This project explores **Deep Knowledge Tracing (DKT)** using **LSTM-based and Transformer-based models** on the **ASSISTments 2017 dataset**. The study evaluates the impact of **interaction encoding strategies, sequence lengths, and sliding window strides** on model performance.  

## ðŸ“Š Key Findings  
- **LSTMs outperform Transformers on shorter sequences** due to better temporal modeling.  
- **Transformers require longer sequences** to leverage self-attention effectively.  
- **Dense overlapping sequences (stride = 1) improve learning** by increasing effective training data.  
- **Combined encoding (single integer) slightly outperforms separate encoding**, reducing redundancy.  

## ðŸš€ Features  
âœ” LSTM-based DKT model implementation  
âœ” Transformer-based Knowledge Tracing model  
âœ” Sequence generation using **sliding window approach**  
âœ” **Early stopping** and **AUC-based evaluation**  
âœ” **Visualization**: Prediction heatmaps and AUROC curves  

## ðŸ“‚ Dataset  
We use the **ASSISTments 2017 dataset**, a widely used benchmark for **Knowledge Tracing research**. It includes:  
- **Student response records**  
- **Skill ID mappings**  
- **Timestamps and correctness labels**  
