{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aDmPpicpKzG"
      },
      "source": [
        "# Deep Knowledge Tracing using RNN (LSTM) model\n",
        "\n",
        "Dataset: Assistments 2017"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9dWWynLpWeg"
      },
      "source": [
        "# Data Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOcSYVLnTTkg"
      },
      "source": [
        "Import Dataset from Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-RKcKY1pdZw",
        "outputId": "de7012fd-2a10-4529-a11c-b63d3adfdd86"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "assistments = pd.read_csv('/content/drive/MyDrive/DeepKT/assistments_2017.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1E2M0FYsQLUm"
      },
      "source": [
        "**Assistments 2017**\n",
        "\n",
        "We will use mainly 2 columns from the dataframe: Skill and Correctness, the other two columns will be for aiding preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "UTLa7NWpQKwY",
        "outputId": "b63f2f87-8bd4-4277-f39b-384b06cb4fa3"
      },
      "outputs": [],
      "source": [
        "assistments[['studentId', 'skill', 'correct', 'action_num']].head(15000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2lEmy8jqRyw"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpaBZTmQqZCO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, List, Dict, Any\n",
        "\n",
        "@dataclass\n",
        "class SequenceConfig:\n",
        "    seq_length: int\n",
        "    sliding_window_step: int = 1\n",
        "    max_students: int = 100\n",
        "\n",
        "class SequenceGenerator:\n",
        "    def __init__(self, config: SequenceConfig, skill_to_id: Dict):\n",
        "        self.config = config  # Configuring the parameters for preprocessing\n",
        "        self.skill_to_id = {}  # Mapping skills to unique IDs\n",
        "        self.num_skills = 0  # Will be set during data loading\n",
        "\n",
        "    def load_and_process(self, file) -> Tuple[pd.DataFrame, int]:\n",
        "        # Load and preprocess data from Dataset\n",
        "        data = file\n",
        "\n",
        "        self.num_skills = data['skill'].nunique()\n",
        "\n",
        "        # Sort by student and action number\n",
        "        data = data.sort_values(by=['studentId', 'action_num'])\n",
        "\n",
        "        # Select limited number of students if specified\n",
        "        selected_students = data['studentId'].unique()[:self.config.max_students]\n",
        "        data = data[data['studentId'].isin(selected_students)]\n",
        "\n",
        "        # Create skill mapping\n",
        "        self.skill_to_id = self.skill_map(data)\n",
        "\n",
        "        return data, self.num_skills\n",
        "\n",
        "    def skill_map(self, data: pd.DataFrame) -> Dict[str, int]:\n",
        "        skill_to_id = {}\n",
        "\n",
        "        for skill in data['skill'].unique():\n",
        "            skill_to_id[skill] = len(skill_to_id)\n",
        "\n",
        "        return skill_to_id\n",
        "\n",
        "    def encode_interaction(self, skill: int, correctness: int) -> List[int]:\n",
        "        \"\"\"\n",
        "        Encode skill and correctness using one-hot encoding\n",
        "        Returns a vector of length (num_skills * 2) where:\n",
        "        - First num_skills positions represent correct responses for each skill\n",
        "        - Last num_skills positions represent incorrect responses for each skill\n",
        "        \"\"\"\n",
        "        # Create a zero vector of length num_skills * 2\n",
        "        encoded = np.zeros(self.num_skills * 2, dtype=int)\n",
        "\n",
        "        # Set the appropriate position to 1\n",
        "        if correctness == 1:\n",
        "            # Correct response for this skill\n",
        "            encoded[skill] = 1\n",
        "        else:\n",
        "            # Incorrect response for this skill\n",
        "            encoded[skill + self.num_skills] = 1\n",
        "\n",
        "        return encoded.tolist()\n",
        "\n",
        "    def generate_label(self, skill: int, correctness: int) -> List[int]:\n",
        "        \"\"\"Create one-hot encoded label vector\"\"\"\n",
        "        # Create a zero vector of length num_skills\n",
        "        label = np.zeros(self.num_skills, dtype=int)\n",
        "\n",
        "        # Set the skill position to correctness value (0 or 1)\n",
        "        label[skill] = correctness\n",
        "\n",
        "        return label.tolist()\n",
        "\n",
        "    def prepare_student_sequences(self, student_data: pd.DataFrame) -> Tuple[List[List[List[int]]], List[List[int]]]:\n",
        "        \"\"\"Prepare sequences for each student\"\"\"\n",
        "        sequences = []\n",
        "        labels = []\n",
        "\n",
        "        if len(student_data) < self.config.seq_length + 1:  # +1 for the next interaction\n",
        "            return sequences, labels\n",
        "\n",
        "        for i in range(0, len(student_data) - self.config.seq_length, self.config.sliding_window_step):\n",
        "            if i + self.config.seq_length >= len(student_data):\n",
        "                break\n",
        "\n",
        "            # Get window of interactions\n",
        "            window = student_data.iloc[i:i + self.config.seq_length]\n",
        "\n",
        "            # Get the next interaction after the sequence\n",
        "            next_interaction = student_data.iloc[i + self.config.seq_length]\n",
        "            next_skill_id = self.skill_to_id[next_interaction['skill']]\n",
        "            next_correctness = next_interaction['correct']\n",
        "\n",
        "            # Encode the sequence\n",
        "            encoded_sequence = [\n",
        "                self.encode_interaction(\n",
        "                    self.skill_to_id[row['skill']],\n",
        "                    row['correct']\n",
        "                ) for _, row in window.iterrows()\n",
        "            ]\n",
        "\n",
        "            # Generate label for the next interaction\n",
        "            label = self.generate_label(next_skill_id, next_correctness)\n",
        "\n",
        "            sequences.append(encoded_sequence)\n",
        "            labels.append(label)\n",
        "\n",
        "        return sequences, labels\n",
        "\n",
        "    def prepare_sequences(self, df: pd.DataFrame) -> Tuple[List[List[List[int]]], List[List[int]]]:\n",
        "        \"\"\"Prepare sequences for all students\"\"\"\n",
        "        all_sequences = []\n",
        "        all_labels = []\n",
        "\n",
        "        for student_id in df['studentId'].unique():\n",
        "            student_data = df[df['studentId'] == student_id]\n",
        "\n",
        "            student_seq, student_lab = self.prepare_student_sequences(student_data)\n",
        "\n",
        "            all_sequences.extend(student_seq)\n",
        "            all_labels.extend(student_lab)\n",
        "\n",
        "        return all_sequences, all_labels\n",
        "\n",
        "config = SequenceConfig(seq_length=10, sliding_window_step=5, max_students=200)\n",
        "generator = SequenceGenerator(config, {})\n",
        "df, num_skills = generator.load_and_process(assistments)\n",
        "seq, lab = generator.prepare_sequences(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_DGB88HlQXu"
      },
      "source": [
        "# **Save and Load Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyuVFtcflVqO",
        "outputId": "00881991-0a5b-4699-9944-24e6470809a1"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import json\n",
        "import os\n",
        "\n",
        "def save_preprocessed_data(sequences, labels, skill_to_id, config, save_dir='/content/drive/MyDrive/DeepKT/preprocessed_data'):\n",
        "    \"\"\"Save preprocessed data to Google Drive\"\"\"\n",
        "    # Mount Google Drive if not already mounted\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Save sequences and labels\n",
        "    np.save(os.path.join(save_dir, 'sequences_10_s5.npy'), np.array(sequences))\n",
        "    np.save(os.path.join(save_dir, 'labels_10_s5.npy'), np.array(labels))\n",
        "\n",
        "    # Save skill mapping and configuration\n",
        "    metadata = {\n",
        "        'skill_to_id': skill_to_id,\n",
        "        'config': {\n",
        "            'seq_length': config.seq_length,\n",
        "            'sliding_window_step': config.sliding_window_step,\n",
        "            'num_students': config.max_students\n",
        "        },\n",
        "        'dataset_stats': {\n",
        "            'num_sequences': len(sequences),\n",
        "            'sequence_length': len(sequences[0]) if sequences else 0,\n",
        "            'num_skills': len(skill_to_id)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(save_dir, 'metadata_10_s5.json'), 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "\n",
        "    print(f\"Data saved successfully to {save_dir}\")\n",
        "    print(\"Files saved:\")\n",
        "    print(f\"- sequences_10_s5.npy: {os.path.getsize(os.path.join(save_dir, 'sequences_10_s5.npy'))/1024/1024:.2f} MB\")\n",
        "    print(f\"- labels_10_s5.npy: {os.path.getsize(os.path.join(save_dir, 'labels_10_s5.npy'))/1024/1024:.2f} MB\")\n",
        "    print(f\"- metadata_10_s5.json: {os.path.getsize(os.path.join(save_dir, 'metadata_10_s5.json'))/1024:.2f} KB\")\n",
        "\n",
        "# # Save Preprocessed Data:\n",
        "save_preprocessed_data(seq, lab, gen.skill_to_id, gen.config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDzwvj9jkPsW"
      },
      "source": [
        "# **In case already preprocessed, load initial packages and start here**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOP9YxcUi22B"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import json\n",
        "import os\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "\n",
        "def load_preprocessed_data(load_dir='/content/drive/MyDrive/DeepKT/preprocessed_data'):\n",
        "    \"\"\"Load preprocessed data from Google Drive\"\"\"\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "    # Load sequences and labels\n",
        "    sequences = np.load(os.path.join(load_dir, 'sequences_10_2.npy'))\n",
        "    labels = np.load(os.path.join(load_dir, 'labels_10_2.npy'))\n",
        "\n",
        "    # Load metadata\n",
        "    with open(os.path.join(load_dir, 'metadata_10_2.json'), 'r') as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    print(\"Data loaded successfully\")\n",
        "    print(f\"Loaded {metadata['dataset_stats']['num_sequences']} sequences\")\n",
        "    print(f\"Sequence length: {metadata['dataset_stats']['sequence_length']}\")\n",
        "    print(f\"Number of skills: {metadata['dataset_stats']['num_skills']}\")\n",
        "\n",
        "    return sequences, labels, metadata\n",
        "\n",
        "# Load in preprocessed data\n",
        "sequences, labels, metadata = load_preprocessed_data()\n",
        "\n",
        "print(sequences[:50])\n",
        "print(labels[:50])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jHKa8nWKRm-"
      },
      "source": [
        "# Data Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFtiyMCZKRWE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def prepare_data(sequences, labels, batch_size = 64, train_ratio = 0.9, val_ratio = 0.01):\n",
        "  sequences = sequences.astype(np.int32)\n",
        "  labels = labels.astype(np.float32)\n",
        "\n",
        "  train_sequences, temp_sequences, train_labels, temp_labels = train_test_split(sequences, labels, train_size=train_ratio, random_state=42)\n",
        "\n",
        "  val_ratio_adjusted = val_ratio / (1 - train_ratio)\n",
        "\n",
        "  val_sequences, test_sequences, val_labels, test_labels = train_test_split(temp_sequences, temp_labels, train_size=val_ratio_adjusted, random_state=42)\n",
        "\n",
        "  def create_dataset(sequences, labels, batch_size, training=False):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((sequences, labels))\n",
        "\n",
        "    if training:\n",
        "      dataset = dataset.shuffle(len(sequences)) # Shuffle tensors\n",
        "\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "    if training:\n",
        "      dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE) # Prefetch for optimum training\n",
        "\n",
        "    return dataset\n",
        "\n",
        "  train_dataset = create_dataset(train_sequences, train_labels, batch_size)\n",
        "  val_dataset = create_dataset(val_sequences, val_labels, batch_size)\n",
        "  test_dataset = create_dataset(test_sequences, test_labels, batch_size)\n",
        "\n",
        "  return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = prepare_data(sequences, labels)\n",
        "\n",
        "def inspect_dataset(dataset, name=\"Dataset\"):\n",
        "    \"\"\"Helper function to inspect the prepared datasets\"\"\"\n",
        "    for sequences, labels in dataset.take(1):\n",
        "        print(f\"\\n{name} inspection:\")\n",
        "        print(f\"Sequences shape: {sequences.shape}\")\n",
        "        print(f\"Labels shape: {labels.shape}\")\n",
        "        print(f\"Sequences dtype: {sequences.dtype}\")\n",
        "        print(f\"Labels dtype: {labels.dtype}\")\n",
        "        print(\"\\nSample sequence (first in batch):\")\n",
        "        print(\"Encoded interactions:\", sequences[0])\n",
        "        print(\"Correctness labels:\", labels[0])\n",
        "\n",
        "inspect_dataset(train_dataset, \"Training\")\n",
        "inspect_dataset(val_dataset, \"Validation\")\n",
        "inspect_dataset(test_dataset, \"Test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J42AT8iQSA98"
      },
      "source": [
        "# LSTM Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0Z-kZIbSGlm"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "\n",
        "class LSTMModel(tf.keras.Model):\n",
        "    def __init__(self, num_skills, seq_len, embed_dim, lstm_units, mlp_units, dropout_rate=0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          num_skills: Number of unique skills in the dataset.\n",
        "          seq_len: The sequence length.\n",
        "          embed_dim: Embedding dimension for skills and correctness combined.\n",
        "          lstm_units: Number of units in the LSTM layer.\n",
        "          mlp_units: A list with the number of units for each MLP Dense layer.\n",
        "          dropout_rate: Dropout rate to use after Dense layers.\n",
        "        \"\"\"\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        # Separate embeddings for skills and correctness\n",
        "        self.skill_embedding = tf.keras.layers.Embedding(num_skills, embed_dim // 2)\n",
        "        self.correctness_embedding = tf.keras.layers.Embedding(2, embed_dim // 2)  # 2 possible values: 0 or 1\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoding = tf.keras.layers.Embedding(seq_len, embed_dim)\n",
        "\n",
        "        # LSTM layer (set return_sequences=True to output a sequence)\n",
        "        self.lstm_layer = tf.keras.layers.LSTM(lstm_units, return_sequences=True)\n",
        "\n",
        "        # MLP layers\n",
        "        self.mlp_layers = []\n",
        "        for mlp_dim in mlp_units:\n",
        "            self.mlp_layers.append(tf.keras.layers.Dense(mlp_dim, activation='relu'))\n",
        "            self.mlp_layers.append(tf.keras.layers.Dropout(dropout_rate))\n",
        "\n",
        "        # Final prediction layer (sigmoid activation for binary prediction)\n",
        "        self.final_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          inputs: Tensor of shape (batch_size, seq_len, 2) where inputs[:,:,0] contains skill IDs\n",
        "                 and inputs[:,:,1] contains correctness values.\n",
        "          training: Boolean flag for dropout behavior.\n",
        "\n",
        "        Returns:\n",
        "          predictions: Tensor of shape (batch_size, seq_len) with probabilities.\n",
        "        \"\"\"\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "\n",
        "        # Split the inputs into skill IDs and correctness values\n",
        "        skill_ids = tf.cast(inputs[:, :, 0], tf.int32)      # (batch_size, seq_len)\n",
        "        correctness = tf.cast(inputs[:, :, 1], tf.int32)    # (batch_size, seq_len)\n",
        "\n",
        "        # Create positional indices\n",
        "        positions = tf.range(start=0, limit=self.seq_len, delta=1)\n",
        "        positions = tf.expand_dims(positions, axis=0)       # Shape: (1, seq_len)\n",
        "        positions = tf.tile(positions, [batch_size, 1])     # Shape: (batch_size, seq_len)\n",
        "\n",
        "        # Get embeddings for skills and correctness\n",
        "        skill_emb = self.skill_embedding(skill_ids)         # (batch_size, seq_len, embed_dim//2)\n",
        "        correctness_emb = self.correctness_embedding(correctness)  # (batch_size, seq_len, embed_dim//2)\n",
        "\n",
        "        # Concatenate skill and correctness embeddings\n",
        "        x = tf.concat([skill_emb, correctness_emb], axis=-1)  # (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        # Add positional encoding\n",
        "        pos_enc = self.pos_encoding(positions)              # (batch_size, seq_len, embed_dim)\n",
        "        x = x + pos_enc\n",
        "\n",
        "        # Process with LSTM\n",
        "        x = self.lstm_layer(x)  # (batch_size, seq_len, lstm_units)\n",
        "\n",
        "        # Pass through the MLP layers\n",
        "        for layer in self.mlp_layers:\n",
        "            if isinstance(layer, tf.keras.layers.Dropout):\n",
        "                x = layer(x, training=training)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "\n",
        "        # Final prediction layer produces shape (batch_size, seq_len, 1)\n",
        "        x = self.final_layer(x)\n",
        "        # Remove last dimension so the output shape becomes (batch_size, seq_len)\n",
        "        return tf.squeeze(x, -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZapEj6i-POMY"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NzoaI476N1V"
      },
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# Training, Evaluation, and Testing Code\n",
        "# ===============================================\n",
        "\n",
        "# ----- Custom Training Step -----\n",
        "@tf.function\n",
        "def train_step(model, optimizer, batch_sequences, batch_labels):\n",
        "    \"\"\"\n",
        "    Performs one training step for a batch.\n",
        "\n",
        "    Args:\n",
        "      model: The LSTM model.\n",
        "      optimizer: An instance of tf.keras.optimizers.\n",
        "      batch_sequences: Tensor of shape (batch_size, seq_len, 2) where [:,:,0] contains skill IDs\n",
        "                       and [:,:,1] contains correctness values.\n",
        "      batch_labels: Tensor of shape (batch_size, seq_len) with ground truth labels.\n",
        "\n",
        "    Returns:\n",
        "      loss: The average loss for the batch.\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Forward pass: get predictions\n",
        "        predictions = model(batch_sequences, training=True)\n",
        "        # In knowledge tracing, we predict the outcome of the *next* step:\n",
        "        # Compare predictions at time t with labels at time t+1\n",
        "        pred = predictions[:, :-1]\n",
        "        target = tf.cast(batch_labels[:, 1:], tf.float32)\n",
        "        loss = tf.keras.losses.binary_crossentropy(target, pred)\n",
        "        loss = tf.reduce_mean(loss)\n",
        "\n",
        "    # Backpropagation and optimizer step\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "# ----- Evaluation Function -----\n",
        "def evaluate_model(model, dataset):\n",
        "    \"\"\"\n",
        "    Evaluates the model over an entire dataset.\n",
        "\n",
        "    Args:\n",
        "      model: The LSTM model.\n",
        "      dataset: A tf.data.Dataset yielding (batch_sequences, batch_labels) where batch_sequences\n",
        "               has shape (batch_size, seq_len, 2) and batch_labels has shape (batch_size, seq_len).\n",
        "\n",
        "    Returns:\n",
        "      auc: ROC-AUC computed over all predictions.\n",
        "      accuracy: Binary accuracy computed over all predictions.\n",
        "    \"\"\"\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch_sequences, batch_labels in dataset:\n",
        "        predictions = model(batch_sequences, training=False)\n",
        "        # Shift predictions and labels so that prediction at time t is compared with label at time t+1\n",
        "        pred = predictions[:, :-1]\n",
        "        target = batch_labels[:, 1:]\n",
        "\n",
        "        all_preds.append(pred)\n",
        "        all_labels.append(target)\n",
        "\n",
        "    # Concatenate outputs across batches\n",
        "    all_preds = tf.concat(all_preds, axis=0)\n",
        "    all_labels = tf.concat(all_labels, axis=0)\n",
        "\n",
        "    # Flatten the tensors to compute scalar metrics\n",
        "    all_preds_np = all_preds.numpy().flatten()\n",
        "    all_labels_np = all_labels.numpy().flatten()\n",
        "\n",
        "    try:\n",
        "        auc = roc_auc_score(all_labels_np, all_preds_np)\n",
        "    except Exception as e:\n",
        "        print(\"Error computing AUC:\", e)\n",
        "        auc = 0.0\n",
        "\n",
        "    y_pred_bin = (all_preds_np > 0.5).astype(int)\n",
        "    accuracy = accuracy_score(all_labels_np, y_pred_bin)\n",
        "\n",
        "    return auc, accuracy\n",
        "\n",
        "# ----- Complete Training Loop -----\n",
        "def train_model(model, train_dataset, val_dataset, test_dataset, epochs=50, patience=10, learning_rate=0.001):\n",
        "    \"\"\"\n",
        "    Trains the model using a custom training loop, printing metrics during training and evaluating on\n",
        "    the training, validation, and test datasets after each epoch.\n",
        "\n",
        "    Args:\n",
        "      model: The LSTM model.\n",
        "      train_dataset: tf.data.Dataset for training.\n",
        "      val_dataset: tf.data.Dataset for validation.\n",
        "      test_dataset: tf.data.Dataset for testing.\n",
        "      epochs: Maximum number of epochs to train.\n",
        "      patience: Number of epochs to wait for improvement before early stopping.\n",
        "      learning_rate: Learning rate for the optimizer.\n",
        "\n",
        "    Returns:\n",
        "      model: The trained model (with the best weights restored).\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "    best_test_auc = 0.0\n",
        "    patience_counter = 0\n",
        "    best_weights = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()\n",
        "        train_loss_metric = tf.keras.metrics.Mean()\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        # Training loop over batches\n",
        "        for batch_idx, (batch_sequences, batch_labels) in enumerate(train_dataset):\n",
        "            loss = train_step(model, optimizer, batch_sequences, batch_labels)\n",
        "            train_loss_metric.update_state(loss)\n",
        "\n",
        "            if (batch_idx + 1) % 50 == 0:\n",
        "                print(f\"  Batch {batch_idx+1} - Loss: {loss:.4f}\")\n",
        "\n",
        "        epoch_loss = train_loss_metric.result().numpy()\n",
        "        print(f\"Epoch {epoch+1} - Average Training Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "        # Evaluate on test dataset\n",
        "        test_auc, test_accuracy = evaluate_model(model, test_dataset)\n",
        "        print(f\"  Test Metrics - AUC: {test_auc:.4f} | Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "        # Early stopping check (based on test AUC)\n",
        "        if test_auc > best_test_auc:\n",
        "            best_test_auc = test_auc\n",
        "            best_weights = model.get_weights()\n",
        "            patience_counter = 0\n",
        "            print(\"  New best model found!\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"  Early stopping triggered!\")\n",
        "                break\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        print(f\"Epoch {epoch+1} took {epoch_time:.2f}s\")\n",
        "\n",
        "    # Restore best model weights if available\n",
        "    if best_weights is not None:\n",
        "        model.set_weights(best_weights)\n",
        "        print(f\"\\nTraining completed. Best Test AUC: {best_test_auc:.4f}\")\n",
        "\n",
        "    final_test_auc, final_test_accuracy = evaluate_model(model, test_dataset)\n",
        "    print(f\"\\nFinal Test Metrics - AUC: {final_test_auc:.4f} | Accuracy: {final_test_accuracy:.4f}\")\n",
        "    return model\n",
        "\n",
        "lstm_model = LSTMModel(\n",
        "    num_skills=len(metadata['skill_to_id']),  # Now using num_skills instead of num_items\n",
        "    seq_len=metadata['config']['seq_length'],\n",
        "    embed_dim=64,\n",
        "    lstm_units=128,\n",
        "    mlp_units=[128, 64],\n",
        "    dropout_rate=0.1\n",
        ")\n",
        "\n",
        "# Train the model (this will print metrics during training and after each epoch):\n",
        "trained_lstm_model = train_model(\n",
        "    model=lstm_model,\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    test_dataset=test_dataset,\n",
        "    epochs=50,\n",
        "    patience=10,\n",
        "    learning_rate=0.001\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKNv-6S00Sus"
      },
      "source": [
        "# Save and Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QU3IQEDg0XsK",
        "outputId": "68a7bcf6-9221-4880-812e-492d543ae300"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# ====================================================\n",
        "# Save the LSTM model weights\n",
        "# ====================================================\n",
        "save_dir = \"/content/drive/MyDrive/DeepKT/saved_models\"\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "lstm_model_save_path = os.path.join(save_dir, \"seq30_trained_lstm_model.weights.h5\")\n",
        "trained_lstm_model.save_weights(lstm_model_save_path)\n",
        "print(f\"Model weights saved to {lstm_model_save_path}\")\n",
        "\n",
        "# ====================================================\n",
        "# Load the LSTM model for evaluation & visualization\n",
        "# ====================================================\n",
        "# Recreate the LSTM model architecture with the same configuration.\n",
        "loaded_lstm_model = LSTMModel(\n",
        "    num_items=2 * len(metadata['skill_to_id']),\n",
        "    seq_len=metadata['config']['seq_length'],\n",
        "    embed_dim=64,\n",
        "    lstm_units=128,\n",
        "    mlp_units=[128, 64],\n",
        "    dropout_rate=0.1\n",
        ")\n",
        "\n",
        "# Build the model by passing a dummy input.\n",
        "dummy_input = tf.zeros((1, metadata['config']['seq_length']))\n",
        "_ = loaded_lstm_model(dummy_input, training=False)\n",
        "\n",
        "# Load the saved weights.\n",
        "loaded_lstm_model.load_weights(lstm_model_save_path)\n",
        "print(f\"Model loaded from {lstm_model_save_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqYLSdpwh_Y5"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "LdEYIOmgiCJO",
        "outputId": "70738e66-0a14-4d02-dfb2-023af1c6b2b5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
        "\n",
        "def plot_roc_curve(model, dataset):\n",
        "    \"\"\"\n",
        "    Generate and display the ROC curve for the given dataset using the LSTM-based model.\n",
        "\n",
        "    For each batch, the predictions (with training=False) are computed and shifted so that\n",
        "    prediction at time t is compared with the label at time t+1. The false positive and true\n",
        "    positive rates are computed over all batches, and the ROC curve (with AUC) is plotted.\n",
        "    \"\"\"\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch_sequences, batch_labels in dataset:\n",
        "        predictions = model(batch_sequences, training=False)\n",
        "        # Remove the last timestep of predictions and the first timestep of labels\n",
        "        pred = predictions[:, :-1]\n",
        "        target = tf.cast(batch_labels[:, 1:], tf.float32)\n",
        "        all_preds.append(pred)\n",
        "        all_labels.append(target)\n",
        "\n",
        "    # Concatenate all batches along the batch dimension.\n",
        "    all_preds = tf.concat(all_preds, axis=0)\n",
        "    all_labels = tf.concat(all_labels, axis=0)\n",
        "\n",
        "    # Flatten to compute scalar metrics.\n",
        "    y_scores = all_preds.numpy().flatten()\n",
        "    y_true = all_labels.numpy().flatten()\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.4f})\", lw=2)\n",
        "    plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", lw=2)\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"Receiver Operating Characteristic (ROC)\")\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def visualize_sample_sequence_squares(model, dataset, sample_index=0, square_size=1, gap=0.2):\n",
        "    \"\"\"\n",
        "    Visualize the predictive performance on a single sequence from the dataset as a sequence of squares.\n",
        "\n",
        "    Each squareâ€™s opacity (green) indicates the prediction accuracy at that time step:\n",
        "      - For a true label of 1, the score is the predicted probability.\n",
        "      - For a true label of 0, the score is 1 minus the predicted probability.\n",
        "\n",
        "    The predictions are shifted such that the prediction at time t is used to predict the outcome at t+1.\n",
        "\n",
        "    Args:\n",
        "      model: The trained LSTM-based model.\n",
        "      dataset: A tf.data.Dataset yielding (batch_sequences, batch_labels).\n",
        "      sample_index: The index of the sequence within the first batch to visualize.\n",
        "      square_size: The side length of each square.\n",
        "      gap: The gap between squares.\n",
        "    \"\"\"\n",
        "    # Retrieve one batch from the dataset.\n",
        "    for batch_sequences, batch_labels in dataset.take(1):\n",
        "        break\n",
        "\n",
        "    batch_sequences_np = batch_sequences.numpy()\n",
        "    batch_labels_np = batch_labels.numpy()\n",
        "\n",
        "    # Ensure sample_index is within the batch.\n",
        "    if sample_index >= batch_sequences_np.shape[0]:\n",
        "        sample_index = 0\n",
        "\n",
        "    # Select the sample sequence and its corresponding labels.\n",
        "    sample_sequence = batch_sequences_np[sample_index:sample_index+1]\n",
        "    sample_labels = batch_labels_np[sample_index]\n",
        "\n",
        "    # Obtain model predictions.\n",
        "    predictions = model(sample_sequence, training=False).numpy()\n",
        "\n",
        "    # Align predictions with labels: prediction at time t is for label at time t+1.\n",
        "    pred_prob = predictions[0, :-1]\n",
        "    true_labels = sample_labels[1:]\n",
        "\n",
        "    # Compute per-timestep \"accuracy\" scores:\n",
        "    # For a true label of 1, the score is the predicted probability;\n",
        "    # For a true label of 0, the score is 1 minus the predicted probability.\n",
        "    scores = np.where(true_labels == 1, pred_prob, 1 - pred_prob)\n",
        "    n = len(scores)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 3))\n",
        "\n",
        "    # Draw squares for each time step with a gap.\n",
        "    for i in range(n):\n",
        "        x = i * (square_size + gap)\n",
        "        alpha = scores[i]\n",
        "        rect = plt.Rectangle((x, 0), square_size, square_size, color='green', alpha=alpha)\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "    total_width = n * square_size + (n - 1) * gap\n",
        "    ax.set_xlim(0, total_width)\n",
        "    ax.set_ylim(0, square_size)\n",
        "    ax.set_aspect('equal')\n",
        "\n",
        "    # Set x-ticks at the center of each square.\n",
        "    ax.set_xticks([i * (square_size + gap) + square_size/2 for i in range(n)])\n",
        "    ax.set_xticklabels(np.arange(1, n+1))\n",
        "    ax.set_yticks([])\n",
        "\n",
        "    ax.set_xlabel(\"Timestep\")\n",
        "    ax.set_title(\"Prediction Accuracy Visualization (Square Opacities)\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# Replace 'trained_lstm_model' with your LSTM-based model variable and 'test_dataset' with your tf.data.Dataset.\n",
        "plot_roc_curve(trained_lstm_model, test_dataset)\n",
        "visualize_sample_sequence_squares(trained_lstm_model, test_dataset, sample_index=0)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
