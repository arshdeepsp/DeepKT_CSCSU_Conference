{"cells":[{"cell_type":"markdown","metadata":{"id":"4aDmPpicpKzG"},"source":["# Deep Knowledge Tracing using Transformer model\n","\n","Dataset: Assistments 2017"]},{"cell_type":"markdown","metadata":{"id":"R9dWWynLpWeg"},"source":["# Data Layer"]},{"cell_type":"markdown","metadata":{"id":"fOcSYVLnTTkg"},"source":["Import Dataset from Drive"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49439,"status":"ok","timestamp":1740960980251,"user":{"displayName":"Arsh Parmar","userId":"16185547295131340337"},"user_tz":480},"id":"x-RKcKY1pdZw","outputId":"2a96df8f-2199-425d-a036-9e4395cc5f4e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-1-fc93e142744a>:6: DtypeWarning: Columns (76,77) have mixed types. Specify dtype option on import or set low_memory=False.\n","  assistments = pd.read_csv('/content/drive/MyDrive/DeepKT/assistments_2017.csv')\n"]}],"source":["import pandas as pd\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","assistments = pd.read_csv('/content/drive/MyDrive/DeepKT/assistments_2017.csv')"]},{"cell_type":"markdown","metadata":{"id":"1E2M0FYsQLUm"},"source":["**Assistments 2017**\n","\n","We will use mainly 2 columns from the dataframe: Skill and Correctness, the other two columns will be for aiding preprocessing."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":351,"status":"ok","timestamp":1740961137917,"user":{"displayName":"Arsh Parmar","userId":"16185547295131340337"},"user_tz":480},"id":"UTLa7NWpQKwY","outputId":"1b1980da-909a-42da-e0f1-2e9d59c3ec72"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["942816"]},"metadata":{},"execution_count":5}],"source":["assistments[['studentId', 'skill', 'correct', 'action_num']].head(15000)\n","\n","assistments['action_num'].nunique()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"H2lEmy8jqRyw"},"source":["# Preprocess"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":151014,"status":"ok","timestamp":1740605918900,"user":{"displayName":"Arsh Parmar","userId":"16185547295131340337"},"user_tz":480},"id":"XpaBZTmQqZCO","outputId":"f81eade4-2e74-48f2-98bd-a3aca1b63c74"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-25-1ad270b9af39>:20: DtypeWarning: Columns (76,77) have mixed types. Specify dtype option on import or set low_memory=False.\n","  data = pd.read_csv(file_path)\n"]},{"output_type":"stream","name":"stdout","text":["Checkpoint 0       8\n","1       8\n","2       8\n","3       8\n","4       8\n","       ..\n","1051    8\n","1052    8\n","1053    8\n","1054    8\n","1055    8\n","Name: studentId, Length: 1056, dtype: int64\n","Checkpoint 1056    35\n","1057    35\n","1058    35\n","1059    35\n","1060    35\n","        ..\n","2044    35\n","2045    35\n","2046    35\n","2047    35\n","2048    35\n","Name: studentId, Length: 993, dtype: int64\n","Checkpoint 2049    39\n","2050    39\n","2051    39\n","2052    39\n","2053    39\n","        ..\n","2462    39\n","2463    39\n","2464    39\n","2465    39\n","2466    39\n","Name: studentId, Length: 418, dtype: int64\n","Checkpoint 2467    64\n","2468    64\n","2469    64\n","2470    64\n","2471    64\n","        ..\n","3881    64\n","3882    64\n","3883    64\n","3884    64\n","3885    64\n","Name: studentId, Length: 1419, dtype: int64\n","Checkpoint 3886    77\n","3887    77\n","3888    77\n","3889    77\n","3890    77\n","        ..\n","4198    77\n","4199    77\n","4200    77\n","4201    77\n","4202    77\n","Name: studentId, Length: 317, dtype: int64\n","Checkpoint 4203    126\n","4204    126\n","4205    126\n","4206    126\n","4207    126\n","       ... \n","4604    126\n","4605    126\n","4606    126\n","4607    126\n","4608    126\n","Name: studentId, Length: 406, dtype: int64\n","Checkpoint 4609    134\n","4610    134\n","4611    134\n","4612    134\n","4613    134\n","       ... \n","4873    134\n","4874    134\n","4875    134\n","4876    134\n","4877    134\n","Name: studentId, Length: 269, dtype: int64\n","Checkpoint 4878    156\n","4879    156\n","4880    156\n","4881    156\n","4882    156\n","       ... \n","5385    156\n","5386    156\n","5387    156\n","5388    156\n","5389    156\n","Name: studentId, Length: 512, dtype: int64\n","Checkpoint 5390    160\n","5391    160\n","5392    160\n","5393    160\n","5394    160\n","       ... \n","6124    160\n","6125    160\n","6126    160\n","6127    160\n","6128    160\n","Name: studentId, Length: 739, dtype: int64\n","Checkpoint 6129    164\n","6130    164\n","6131    164\n","6132    164\n","6133    164\n","       ... \n","6303    164\n","6304    164\n","6305    164\n","6306    164\n","6307    164\n","Name: studentId, Length: 179, dtype: int64\n","Checkpoint 6308    205\n","6309    205\n","6310    205\n","6311    205\n","6312    205\n","       ... \n","6880    205\n","6881    205\n","6882    205\n","6883    205\n","6884    205\n","Name: studentId, Length: 577, dtype: int64\n","Checkpoint 6885    215\n","6886    215\n","6887    215\n","6888    215\n","6889    215\n","       ... \n","7414    215\n","7415    215\n","7416    215\n","7417    215\n","7418    215\n","Name: studentId, Length: 534, dtype: int64\n","Checkpoint 7419    243\n","7420    243\n","7421    243\n","7422    243\n","7423    243\n","       ... \n","7965    243\n","7966    243\n","7967    243\n","7968    243\n","7969    243\n","Name: studentId, Length: 551, dtype: int64\n","Checkpoint 7970    255\n","7971    255\n","7972    255\n","7973    255\n","7974    255\n","       ... \n","9214    255\n","9215    255\n","9216    255\n","9217    255\n","9218    255\n","Name: studentId, Length: 1249, dtype: int64\n","Checkpoint 9219     261\n","9220     261\n","9221     261\n","9222     261\n","9223     261\n","        ... \n","10807    261\n","10808    261\n","10809    261\n","10810    261\n","10811    261\n","Name: studentId, Length: 1593, dtype: int64\n","Checkpoint 10812    283\n","10813    283\n","10814    283\n","10815    283\n","10816    283\n","        ... \n","12018    283\n","12019    283\n","12020    283\n","12021    283\n","12022    283\n","Name: studentId, Length: 1211, dtype: int64\n","Checkpoint 12023    285\n","12024    285\n","12025    285\n","12026    285\n","12027    285\n","        ... \n","12816    285\n","12817    285\n","12818    285\n","12819    285\n","12820    285\n","Name: studentId, Length: 798, dtype: int64\n","Checkpoint 12821    291\n","12822    291\n","12823    291\n","12824    291\n","12825    291\n","        ... \n","13530    291\n","13531    291\n","13532    291\n","13533    291\n","13534    291\n","Name: studentId, Length: 714, dtype: int64\n","Checkpoint 13535    299\n","13536    299\n","13537    299\n","13538    299\n","13539    299\n","        ... \n","14404    299\n","14405    299\n","14406    299\n","14407    299\n","14408    299\n","Name: studentId, Length: 874, dtype: int64\n","Checkpoint 14409    337\n","14410    337\n","14411    337\n","14412    337\n","14413    337\n","        ... \n","15120    337\n","15121    337\n","15122    337\n","15123    337\n","15124    337\n","Name: studentId, Length: 716, dtype: int64\n","Checkpoint 15125    344\n","15126    344\n","15127    344\n","15128    344\n","15129    344\n","        ... \n","15571    344\n","15572    344\n","15573    344\n","15574    344\n","15575    344\n","Name: studentId, Length: 451, dtype: int64\n","Checkpoint 15576    349\n","15577    349\n","15578    349\n","15579    349\n","15580    349\n","        ... \n","16682    349\n","16683    349\n","16684    349\n","16685    349\n","16686    349\n","Name: studentId, Length: 1111, dtype: int64\n","Checkpoint 16687    360\n","16688    360\n","16689    360\n","16690    360\n","16691    360\n","        ... \n","17850    360\n","17851    360\n","17852    360\n","17853    360\n","17854    360\n","Name: studentId, Length: 1168, dtype: int64\n","Checkpoint 17855    372\n","17856    372\n","17857    372\n","17858    372\n","17859    372\n","        ... \n","18012    372\n","18013    372\n","18014    372\n","18015    372\n","18016    372\n","Name: studentId, Length: 162, dtype: int64\n","Checkpoint 18017    383\n","18018    383\n","18019    383\n","18020    383\n","18021    383\n","        ... \n","19270    383\n","19271    383\n","19272    383\n","19273    383\n","19274    383\n","Name: studentId, Length: 1258, dtype: int64\n","Checkpoint 19275    391\n","19276    391\n","19277    391\n","19278    391\n","19279    391\n","        ... \n","20572    391\n","20573    391\n","20574    391\n","20575    391\n","20576    391\n","Name: studentId, Length: 1302, dtype: int64\n","Checkpoint 20577    401\n","20578    401\n","20579    401\n","20580    401\n","20581    401\n","        ... \n","21138    401\n","21139    401\n","21140    401\n","21141    401\n","21142    401\n","Name: studentId, Length: 566, dtype: int64\n","Checkpoint 21143    404\n","21144    404\n","21145    404\n","21146    404\n","21147    404\n","        ... \n","21552    404\n","21553    404\n","21554    404\n","21555    404\n","21556    404\n","Name: studentId, Length: 414, dtype: int64\n","Checkpoint 21557    410\n","21558    410\n","21559    410\n","21560    410\n","21561    410\n","        ... \n","22210    410\n","22211    410\n","22212    410\n","22213    410\n","22214    410\n","Name: studentId, Length: 658, dtype: int64\n","Checkpoint 22215    418\n","22216    418\n","22217    418\n","22218    418\n","22219    418\n","        ... \n","22686    418\n","22687    418\n","22688    418\n","22689    418\n","22690    418\n","Name: studentId, Length: 476, dtype: int64\n","Checkpoint 22691    447\n","22692    447\n","22693    447\n","22694    447\n","22695    447\n","        ... \n","22992    447\n","22993    447\n","22994    447\n","22995    447\n","22996    447\n","Name: studentId, Length: 306, dtype: int64\n","Checkpoint 22997    473\n","22998    473\n","22999    473\n","23000    473\n","23001    473\n","        ... \n","23716    473\n","23717    473\n","23718    473\n","23719    473\n","23720    473\n","Name: studentId, Length: 724, dtype: int64\n","Checkpoint 23721    474\n","23722    474\n","23723    474\n","23724    474\n","23725    474\n","        ... \n","24600    474\n","24601    474\n","24602    474\n","24603    474\n","24604    474\n","Name: studentId, Length: 884, dtype: int64\n","Checkpoint 24605    475\n","24606    475\n","24607    475\n","24608    475\n","24609    475\n","        ... \n","25359    475\n","25360    475\n","25361    475\n","25362    475\n","25363    475\n","Name: studentId, Length: 759, dtype: int64\n","Checkpoint 25364    476\n","25365    476\n","25366    476\n","25367    476\n","25368    476\n","        ... \n","26673    476\n","26674    476\n","26675    476\n","26676    476\n","26677    476\n","Name: studentId, Length: 1314, dtype: int64\n","Checkpoint 26678    492\n","26679    492\n","26680    492\n","26681    492\n","26682    492\n","        ... \n","27280    492\n","27281    492\n","27282    492\n","27283    492\n","27284    492\n","Name: studentId, Length: 607, dtype: int64\n","Checkpoint 27285    496\n","27286    496\n","27287    496\n","27288    496\n","27289    496\n","        ... \n","27817    496\n","27818    496\n","27819    496\n","27820    496\n","27821    496\n","Name: studentId, Length: 537, dtype: int64\n","Checkpoint 27822    512\n","27823    512\n","27824    512\n","27825    512\n","27826    512\n","        ... \n","28405    512\n","28406    512\n","28407    512\n","28408    512\n","28409    512\n","Name: studentId, Length: 588, dtype: int64\n","Checkpoint 28410    530\n","28411    530\n","28412    530\n","28413    530\n","28414    530\n","        ... \n","30262    530\n","30263    530\n","30264    530\n","30265    530\n","30266    530\n","Name: studentId, Length: 1857, dtype: int64\n","Checkpoint 30267    532\n","30268    532\n","30269    532\n","30270    532\n","30271    532\n","        ... \n","30392    532\n","30393    532\n","30394    532\n","30395    532\n","30396    532\n","Name: studentId, Length: 130, dtype: int64\n","Checkpoint 30397    542\n","30398    542\n","30399    542\n","30400    542\n","30401    542\n","        ... \n","31406    542\n","31407    542\n","31408    542\n","31409    542\n","31410    542\n","Name: studentId, Length: 1014, dtype: int64\n","Checkpoint 31411    546\n","31412    546\n","31413    546\n","31414    546\n","31415    546\n","        ... \n","32212    546\n","32213    546\n","32214    546\n","32215    546\n","32216    546\n","Name: studentId, Length: 806, dtype: int64\n","Checkpoint 32217    566\n","32218    566\n","32219    566\n","32220    566\n","32221    566\n","        ... \n","32535    566\n","32536    566\n","32537    566\n","32538    566\n","32539    566\n","Name: studentId, Length: 323, dtype: int64\n","Checkpoint 32540    568\n","32541    568\n","32542    568\n","32543    568\n","32544    568\n","        ... \n","33448    568\n","33449    568\n","33450    568\n","33451    568\n","33452    568\n","Name: studentId, Length: 913, dtype: int64\n","Checkpoint 33453    603\n","33454    603\n","33455    603\n","33456    603\n","33457    603\n","        ... \n","33849    603\n","33850    603\n","33851    603\n","33852    603\n","33853    603\n","Name: studentId, Length: 401, dtype: int64\n","Checkpoint 33854    609\n","33855    609\n","33856    609\n","33857    609\n","33858    609\n","        ... \n","34114    609\n","34115    609\n","34116    609\n","34117    609\n","34118    609\n","Name: studentId, Length: 265, dtype: int64\n","Checkpoint 34119    616\n","34120    616\n","34121    616\n","34122    616\n","34123    616\n","        ... \n","34359    616\n","34360    616\n","34361    616\n","34362    616\n","34363    616\n","Name: studentId, Length: 245, dtype: int64\n","Checkpoint 34364    621\n","34365    621\n","34366    621\n","34367    621\n","34368    621\n","        ... \n","35018    621\n","35019    621\n","35020    621\n","35021    621\n","35022    621\n","Name: studentId, Length: 659, dtype: int64\n","Checkpoint 35023    631\n","35024    631\n","35025    631\n","35026    631\n","35027    631\n","        ... \n","35466    631\n","35467    631\n","35468    631\n","35469    631\n","35470    631\n","Name: studentId, Length: 448, dtype: int64\n","Checkpoint 35471    633\n","35472    633\n","35473    633\n","35474    633\n","35475    633\n","        ... \n","37035    633\n","37036    633\n","37037    633\n","37038    633\n","37039    633\n","Name: studentId, Length: 1569, dtype: int64\n","Checkpoint 37040    636\n","37041    636\n","37042    636\n","37043    636\n","37044    636\n","        ... \n","37600    636\n","37601    636\n","37602    636\n","37603    636\n","37604    636\n","Name: studentId, Length: 565, dtype: int64\n","Checkpoint 37605    654\n","37606    654\n","37607    654\n","37608    654\n","37609    654\n","        ... \n","39068    654\n","39069    654\n","39070    654\n","39071    654\n","39072    654\n","Name: studentId, Length: 1468, dtype: int64\n","Checkpoint 39073    663\n","39074    663\n","39075    663\n","39076    663\n","39077    663\n","        ... \n","40251    663\n","40252    663\n","40253    663\n","40254    663\n","40255    663\n","Name: studentId, Length: 1183, dtype: int64\n","Checkpoint 40256    667\n","40257    667\n","40258    667\n","40259    667\n","40260    667\n","        ... \n","40502    667\n","40503    667\n","40504    667\n","40505    667\n","40506    667\n","Name: studentId, Length: 251, dtype: int64\n","Checkpoint 40507    672\n","40508    672\n","40509    672\n","40510    672\n","40511    672\n","        ... \n","41729    672\n","41730    672\n","41731    672\n","41732    672\n","41733    672\n","Name: studentId, Length: 1227, dtype: int64\n","Checkpoint 41734    676\n","41735    676\n","41736    676\n","41737    676\n","41738    676\n","        ... \n","42483    676\n","42484    676\n","42485    676\n","42486    676\n","42487    676\n","Name: studentId, Length: 754, dtype: int64\n","Checkpoint 42488    683\n","42489    683\n","42490    683\n","42491    683\n","42492    683\n","        ... \n","43184    683\n","43185    683\n","43186    683\n","43187    683\n","43188    683\n","Name: studentId, Length: 701, dtype: int64\n","Checkpoint 43189    684\n","43190    684\n","43191    684\n","43192    684\n","43193    684\n","        ... \n","44067    684\n","44068    684\n","44069    684\n","44070    684\n","44071    684\n","Name: studentId, Length: 883, dtype: int64\n","Checkpoint 44072    696\n","44073    696\n","44074    696\n","44075    696\n","44076    696\n","        ... \n","44563    696\n","44564    696\n","44565    696\n","44566    696\n","44567    696\n","Name: studentId, Length: 496, dtype: int64\n","Checkpoint 44568    709\n","44569    709\n","44570    709\n","44571    709\n","44572    709\n","        ... \n","45944    709\n","45945    709\n","45946    709\n","45947    709\n","45948    709\n","Name: studentId, Length: 1381, dtype: int64\n","Checkpoint 45949    743\n","45950    743\n","45951    743\n","45952    743\n","45953    743\n","        ... \n","47254    743\n","47255    743\n","47256    743\n","47257    743\n","47258    743\n","Name: studentId, Length: 1310, dtype: int64\n","Checkpoint 47259    752\n","47260    752\n","47261    752\n","47262    752\n","47263    752\n","        ... \n","48031    752\n","48032    752\n","48033    752\n","48034    752\n","48035    752\n","Name: studentId, Length: 777, dtype: int64\n","Checkpoint 48036    781\n","48037    781\n","48038    781\n","48039    781\n","48040    781\n","        ... \n","49015    781\n","49016    781\n","49017    781\n","49018    781\n","49019    781\n","Name: studentId, Length: 984, dtype: int64\n","Checkpoint 49020    782\n","49021    782\n","49022    782\n","49023    782\n","49024    782\n","        ... \n","49761    782\n","49762    782\n","49763    782\n","49764    782\n","49765    782\n","Name: studentId, Length: 746, dtype: int64\n","Checkpoint 49766    789\n","49767    789\n","49768    789\n","49769    789\n","49770    789\n","        ... \n","50611    789\n","50612    789\n","50613    789\n","50614    789\n","50615    789\n","Name: studentId, Length: 850, dtype: int64\n","Checkpoint 50616    808\n","50617    808\n","50618    808\n","50619    808\n","50620    808\n","        ... \n","51463    808\n","51464    808\n","51465    808\n","51466    808\n","51467    808\n","Name: studentId, Length: 852, dtype: int64\n","Checkpoint 51468    809\n","51469    809\n","51470    809\n","51471    809\n","51472    809\n","        ... \n","52848    809\n","52849    809\n","52850    809\n","52851    809\n","52852    809\n","Name: studentId, Length: 1385, dtype: int64\n","Checkpoint 52853    810\n","52854    810\n","52855    810\n","52856    810\n","52857    810\n","        ... \n","53596    810\n","53597    810\n","53598    810\n","53599    810\n","53600    810\n","Name: studentId, Length: 748, dtype: int64\n","Checkpoint 53601    824\n","53602    824\n","53603    824\n","53604    824\n","53605    824\n","        ... \n","54403    824\n","54404    824\n","54405    824\n","54406    824\n","54407    824\n","Name: studentId, Length: 807, dtype: int64\n","Checkpoint 54408    826\n","54409    826\n","54410    826\n","54411    826\n","54412    826\n","        ... \n","55064    826\n","55065    826\n","55066    826\n","55067    826\n","55068    826\n","Name: studentId, Length: 661, dtype: int64\n","Checkpoint 55069    828\n","55070    828\n","55071    828\n","55072    828\n","55073    828\n","        ... \n","56554    828\n","56555    828\n","56556    828\n","56557    828\n","56558    828\n","Name: studentId, Length: 1490, dtype: int64\n","Checkpoint 56559    832\n","56560    832\n","56561    832\n","56562    832\n","56563    832\n","        ... \n","58320    832\n","58321    832\n","58322    832\n","58323    832\n","58324    832\n","Name: studentId, Length: 1766, dtype: int64\n","Checkpoint 58325    834\n","58326    834\n","58327    834\n","58328    834\n","58329    834\n","        ... \n","58621    834\n","58622    834\n","58623    834\n","58624    834\n","58625    834\n","Name: studentId, Length: 301, dtype: int64\n","Checkpoint 58626    837\n","58627    837\n","58628    837\n","58629    837\n","58630    837\n","        ... \n","59991    837\n","59992    837\n","59993    837\n","59994    837\n","59995    837\n","Name: studentId, Length: 1370, dtype: int64\n","Checkpoint 59996    842\n","59997    842\n","59998    842\n","59999    842\n","60000    842\n","        ... \n","61076    842\n","61077    842\n","61078    842\n","61079    842\n","61080    842\n","Name: studentId, Length: 1085, dtype: int64\n","Checkpoint 61081    843\n","61082    843\n","61083    843\n","61084    843\n","61085    843\n","        ... \n","61915    843\n","61916    843\n","61917    843\n","61918    843\n","61919    843\n","Name: studentId, Length: 839, dtype: int64\n","Checkpoint 61920    848\n","61921    848\n","61922    848\n","61923    848\n","61924    848\n","        ... \n","62555    848\n","62556    848\n","62557    848\n","62558    848\n","62559    848\n","Name: studentId, Length: 640, dtype: int64\n","Checkpoint 62560    868\n","62561    868\n","62562    868\n","62563    868\n","62564    868\n","        ... \n","63312    868\n","63313    868\n","63314    868\n","63315    868\n","63316    868\n","Name: studentId, Length: 757, dtype: int64\n","Checkpoint 63317    898\n","63318    898\n","63319    898\n","63320    898\n","63321    898\n","        ... \n","64431    898\n","64432    898\n","64433    898\n","64434    898\n","64435    898\n","Name: studentId, Length: 1119, dtype: int64\n","Checkpoint 64436    937\n","64437    937\n","64438    937\n","64439    937\n","64440    937\n","        ... \n","64858    937\n","64859    937\n","64860    937\n","64861    937\n","64862    937\n","Name: studentId, Length: 427, dtype: int64\n","Checkpoint 64863    954\n","64864    954\n","64865    954\n","64866    954\n","64867    954\n","        ... \n","67354    954\n","67355    954\n","67356    954\n","67357    954\n","67358    954\n","Name: studentId, Length: 2496, dtype: int64\n","Checkpoint 67359    968\n","67360    968\n","67361    968\n","67362    968\n","67363    968\n","        ... \n","68086    968\n","68087    968\n","68088    968\n","68089    968\n","68090    968\n","Name: studentId, Length: 732, dtype: int64\n","Checkpoint 68091    969\n","68092    969\n","68093    969\n","68094    969\n","68095    969\n","        ... \n","68678    969\n","68679    969\n","68680    969\n","68681    969\n","68682    969\n","Name: studentId, Length: 592, dtype: int64\n","Checkpoint 68683    998\n","68684    998\n","68685    998\n","68686    998\n","68687    998\n","        ... \n","69677    998\n","69678    998\n","69679    998\n","69680    998\n","69681    998\n","Name: studentId, Length: 999, dtype: int64\n","Checkpoint 69682    1007\n","69683    1007\n","69684    1007\n","69685    1007\n","69686    1007\n","         ... \n","70054    1007\n","70055    1007\n","70056    1007\n","70057    1007\n","70058    1007\n","Name: studentId, Length: 377, dtype: int64\n","Checkpoint 70059    1016\n","70060    1016\n","70061    1016\n","70062    1016\n","70063    1016\n","         ... \n","70463    1016\n","70464    1016\n","70465    1016\n","70466    1016\n","70467    1016\n","Name: studentId, Length: 409, dtype: int64\n","Checkpoint 70468    1020\n","70469    1020\n","70470    1020\n","70471    1020\n","70472    1020\n","         ... \n","71394    1020\n","71395    1020\n","71396    1020\n","71397    1020\n","71398    1020\n","Name: studentId, Length: 931, dtype: int64\n","Checkpoint 71399    1044\n","71400    1044\n","71401    1044\n","71402    1044\n","71403    1044\n","         ... \n","71506    1044\n","71507    1044\n","71508    1044\n","71509    1044\n","71510    1044\n","Name: studentId, Length: 112, dtype: int64\n","Checkpoint 71511    1046\n","71512    1046\n","71513    1046\n","71514    1046\n","71515    1046\n","         ... \n","72558    1046\n","72559    1046\n","72560    1046\n","72561    1046\n","72562    1046\n","Name: studentId, Length: 1052, dtype: int64\n","Checkpoint 72563    1047\n","72564    1047\n","72565    1047\n","72566    1047\n","72567    1047\n","         ... \n","73064    1047\n","73065    1047\n","73066    1047\n","73067    1047\n","73068    1047\n","Name: studentId, Length: 506, dtype: int64\n","Checkpoint 73069    1055\n","73070    1055\n","73071    1055\n","73072    1055\n","73073    1055\n","         ... \n","73890    1055\n","73891    1055\n","73892    1055\n","73893    1055\n","73894    1055\n","Name: studentId, Length: 826, dtype: int64\n","Checkpoint 73895    1079\n","73896    1079\n","73897    1079\n","73898    1079\n","73899    1079\n","         ... \n","76483    1079\n","76484    1079\n","76485    1079\n","76486    1079\n","76487    1079\n","Name: studentId, Length: 2593, dtype: int64\n","Checkpoint 76488    1107\n","76489    1107\n","76490    1107\n","76491    1107\n","76492    1107\n","         ... \n","77798    1107\n","77799    1107\n","77800    1107\n","77801    1107\n","77802    1107\n","Name: studentId, Length: 1315, dtype: int64\n","Checkpoint 77803    1108\n","77804    1108\n","77805    1108\n","77806    1108\n","77807    1108\n","         ... \n","78286    1108\n","78287    1108\n","78288    1108\n","78289    1108\n","78290    1108\n","Name: studentId, Length: 488, dtype: int64\n","Checkpoint 78291    1110\n","78292    1110\n","78293    1110\n","78294    1110\n","78295    1110\n","         ... \n","79366    1110\n","79367    1110\n","79368    1110\n","79369    1110\n","79370    1110\n","Name: studentId, Length: 1080, dtype: int64\n","Checkpoint 79371    1178\n","79372    1178\n","79373    1178\n","79374    1178\n","79375    1178\n","         ... \n","79857    1178\n","79858    1178\n","79859    1178\n","79860    1178\n","79861    1178\n","Name: studentId, Length: 491, dtype: int64\n","Checkpoint 79862    1202\n","79863    1202\n","79864    1202\n","79865    1202\n","79866    1202\n","         ... \n","80572    1202\n","80573    1202\n","80574    1202\n","80575    1202\n","80576    1202\n","Name: studentId, Length: 715, dtype: int64\n","Checkpoint 80577    1208\n","80578    1208\n","80579    1208\n","80580    1208\n","80581    1208\n","         ... \n","81666    1208\n","81667    1208\n","81668    1208\n","81669    1208\n","81670    1208\n","Name: studentId, Length: 1094, dtype: int64\n","Checkpoint 81671    1232\n","81672    1232\n","81673    1232\n","81674    1232\n","81675    1232\n","         ... \n","81893    1232\n","81894    1232\n","81895    1232\n","81896    1232\n","81897    1232\n","Name: studentId, Length: 227, dtype: int64\n","Checkpoint 81898    1237\n","81899    1237\n","81900    1237\n","81901    1237\n","81902    1237\n","         ... \n","82382    1237\n","82383    1237\n","82384    1237\n","82385    1237\n","82386    1237\n","Name: studentId, Length: 489, dtype: int64\n","Checkpoint 82387    1238\n","82388    1238\n","82389    1238\n","82390    1238\n","82391    1238\n","         ... \n","82684    1238\n","82685    1238\n","82686    1238\n","82687    1238\n","82688    1238\n","Name: studentId, Length: 302, dtype: int64\n","Checkpoint 82689    1240\n","82690    1240\n","82691    1240\n","82692    1240\n","82693    1240\n","         ... \n","83309    1240\n","83310    1240\n","83311    1240\n","83312    1240\n","83313    1240\n","Name: studentId, Length: 625, dtype: int64\n","Checkpoint 83314    1250\n","83315    1250\n","83316    1250\n","83317    1250\n","83318    1250\n","         ... \n","83803    1250\n","83804    1250\n","83805    1250\n","83806    1250\n","83807    1250\n","Name: studentId, Length: 494, dtype: int64\n","Checkpoint 83808    1287\n","83809    1287\n","83810    1287\n","83811    1287\n","83812    1287\n","         ... \n","84649    1287\n","84650    1287\n","84651    1287\n","84652    1287\n","84653    1287\n","Name: studentId, Length: 846, dtype: int64\n","Checkpoint 84654    1294\n","84655    1294\n","84656    1294\n","84657    1294\n","84658    1294\n","         ... \n","85903    1294\n","85904    1294\n","85905    1294\n","85906    1294\n","85907    1294\n","Name: studentId, Length: 1254, dtype: int64\n","Checkpoint 85908    1305\n","85909    1305\n","85910    1305\n","85911    1305\n","85912    1305\n","         ... \n","86458    1305\n","86459    1305\n","86460    1305\n","86461    1305\n","86462    1305\n","Name: studentId, Length: 555, dtype: int64\n","Checkpoint 86463    1313\n","86464    1313\n","86465    1313\n","86466    1313\n","86467    1313\n","         ... \n","87940    1313\n","87941    1313\n","87942    1313\n","87943    1313\n","87944    1313\n","Name: studentId, Length: 1482, dtype: int64\n","Checkpoint 87945    1317\n","87946    1317\n","87947    1317\n","87948    1317\n","87949    1317\n","         ... \n","89180    1317\n","89181    1317\n","89182    1317\n","89183    1317\n","89184    1317\n","Name: studentId, Length: 1240, dtype: int64\n","Checkpoint 89185    1337\n","89186    1337\n","89187    1337\n","89188    1337\n","89189    1337\n","         ... \n","89306    1337\n","89307    1337\n","89308    1337\n","89309    1337\n","89310    1337\n","Name: studentId, Length: 126, dtype: int64\n","Checkpoint 89311    1338\n","89312    1338\n","89313    1338\n","89314    1338\n","89315    1338\n","         ... \n","90413    1338\n","90414    1338\n","90415    1338\n","90416    1338\n","90417    1338\n","Name: studentId, Length: 1107, dtype: int64\n","Checkpoint 90418    1348\n","90419    1348\n","90420    1348\n","90421    1348\n","90422    1348\n","         ... \n","91114    1348\n","91115    1348\n","91116    1348\n","91117    1348\n","91118    1348\n","Name: studentId, Length: 701, dtype: int64\n","Checkpoint 91119    1362\n","91120    1362\n","91121    1362\n","91122    1362\n","91123    1362\n","         ... \n","91915    1362\n","91916    1362\n","91917    1362\n","91918    1362\n","91919    1362\n","Name: studentId, Length: 801, dtype: int64\n","Checkpoint 91920    1375\n","91921    1375\n","91922    1375\n","91923    1375\n","91924    1375\n","         ... \n","92496    1375\n","92497    1375\n","92498    1375\n","92499    1375\n","92500    1375\n","Name: studentId, Length: 581, dtype: int64\n","Checkpoint 92501    1380\n","92502    1380\n","92503    1380\n","92504    1380\n","92505    1380\n","         ... \n","93277    1380\n","93278    1380\n","93279    1380\n","93280    1380\n","93281    1380\n","Name: studentId, Length: 781, dtype: int64\n","Checkpoint 93282    1394\n","93283    1394\n","93284    1394\n","93285    1394\n","93286    1394\n","         ... \n","95042    1394\n","95043    1394\n","95044    1394\n","95045    1394\n","95046    1394\n","Name: studentId, Length: 1765, dtype: int64\n","Checkpoint 95047    1404\n","95048    1404\n","95049    1404\n","95050    1404\n","95051    1404\n","         ... \n","95793    1404\n","95794    1404\n","95795    1404\n","95796    1404\n","95797    1404\n","Name: studentId, Length: 751, dtype: int64\n","Checkpoint 95798    1408\n","95799    1408\n","95800    1408\n","95801    1408\n","95802    1408\n","         ... \n","96615    1408\n","96616    1408\n","96617    1408\n","96618    1408\n","96619    1408\n","Name: studentId, Length: 822, dtype: int64\n","Checkpoint 96620    1417\n","96621    1417\n","96622    1417\n","96623    1417\n","96624    1417\n","         ... \n","96901    1417\n","96902    1417\n","96903    1417\n","96904    1417\n","96905    1417\n","Name: studentId, Length: 286, dtype: int64\n","Checkpoint 96906    1437\n","96907    1437\n","96908    1437\n","96909    1437\n","96910    1437\n","         ... \n","97066    1437\n","97067    1437\n","97068    1437\n","97069    1437\n","97070    1437\n","Name: studentId, Length: 165, dtype: int64\n","Checkpoint 97071    1438\n","97072    1438\n","97073    1438\n","97074    1438\n","97075    1438\n","         ... \n","98405    1438\n","98406    1438\n","98407    1438\n","98408    1438\n","98409    1438\n","Name: studentId, Length: 1339, dtype: int64\n"]}],"source":["import pandas as pd\n","import numpy as np\n","\n","from dataclasses import dataclass\n","from typing import Tuple, List, Dict\n","\n","@dataclass\n","class SequenceConfig:\n","  seq_length: int\n","  sliding_window_step: int = 1\n","  max_students: int = 500\n","\n","class SequenceGenerator:\n","  def __init__(self, config: SequenceConfig, skill_to_id: Dict):\n","    self.config = config # Configuring the parameters for preprocessing\n","    self.skill_to_id = {} # Mapping skills to unique IDs\n","\n","  def load_and_process(self, file_path: str) -> Tuple[pd.DataFrame, int]:\n","    # Load and preprocess data from Dataset\n","    data = pd.read_csv(file_path)\n","\n","    num_skills = data['skill'].nunique()\n","\n","    data.sort_values(by=['studentId', 'action_num'])\n","\n","    selected_students = data['studentId'].unique()[:self.config.max_students]\n","    data = data[data['studentId'].isin(selected_students)]\n","\n","    self.skill_to_id = self.skill_map(data)\n","\n","    return data, num_skills\n","\n","  def skill_map(self, data: pd.DataFrame) -> Dict[str, int]:\n","    skill_to_id = {}\n","\n","    for skill in data['skill'].unique():\n","      skill_to_id[skill] = len(skill_to_id)\n","\n","    return skill_to_id\n","\n","  def encode_interaction(self, skill: int, correctness: int) -> int:\n","    # Encode each possible interaction uniquely as a number\n","    return 2 * skill + correctness\n","\n","  def generate_label(self, num_skills: int, skill: int, correctness: int) -> List[int]:\n","    # Create labels to calculate BCE loss\n","    label = np.zeros(num_skills)\n","    label[skill] = correctness\n","    return label\n","\n","  def prepare_student_sequences(self, student_data: pd.DataFrame, num_skills: int) -> Tuple[List, List]:\n","    # Prepare sequences for each student\n","    sequences = []\n","    labels = []\n","\n","    print('Checkpoint', student_data['studentId'])\n","\n","    if len(student_data) < self.config.seq_length:\n","      return sequences, labels\n","\n","    for i in range(0, len(student_data) - self.config.seq_length, self.config.sliding_window_step):\n","      if i + self.config.seq_length > len(student_data):\n","        break\n","\n","      window = student_data.iloc[i: i + self.config.seq_length]\n","\n","      next_interaction = student_data.iloc[i + self.config.seq_length]\n","\n","      encoded_sequence = [self.encode_interaction(self.skill_to_id[row['skill']], row['correct']) for _, row in window.iterrows()]\n","\n","      sequences.append(encoded_sequence)\n","\n","      labels.append(window['correct'].tolist())\n","\n","    return sequences, labels\n","\n","  def prepare_sequences(self, df: pd.DataFrame, num_skills: int) -> Tuple[List, List]:\n","    all_sequences = []\n","    all_labels = []\n","\n","    for student_id in df['studentId'].unique():\n","      student_data = df[df['studentId'] == student_id]\n","\n","      student_seq, student_lab = self.prepare_student_sequences(student_data, num_skills)\n","\n","      all_sequences.extend(student_seq)\n","      all_labels.extend(student_lab)\n","\n","    return all_sequences, all_labels\n","\n","gen = SequenceGenerator(SequenceConfig(30, 1, 120), {})\n","\n","df, num_skills = gen.load_and_process('/content/drive/MyDrive/DeepKT/assistments_2017.csv')\n","\n","df.head(100)\n","\n","seq, lab = gen.prepare_sequences(df, num_skills)\n"]},{"cell_type":"markdown","metadata":{"id":"x_DGB88HlQXu"},"source":["# **Save and Load Functions**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":431,"status":"ok","timestamp":1740605943779,"user":{"displayName":"Arsh Parmar","userId":"16185547295131340337"},"user_tz":480},"id":"iyuVFtcflVqO","outputId":"5a87d7e2-ed0f-41b7-e271-03860bd0eaf1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Data saved successfully to /content/drive/MyDrive/DeepKT/preprocessed_data\n","Files saved:\n","- sequences_30.npy: 21.70 MB\n","- labels_30.npy: 21.70 MB\n","- metadata_30.json: 2.85 KB\n"]}],"source":["import pickle\n","import json\n","import os\n","\n","def save_preprocessed_data(sequences, labels, skill_to_id, config, save_dir='/content/drive/MyDrive/DeepKT/preprocessed_data'):\n","    \"\"\"Save preprocessed data to Google Drive\"\"\"\n","    # Mount Google Drive if not already mounted\n","    if not os.path.exists('/content/drive'):\n","        drive.mount('/content/drive')\n","\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    # Save sequences and labels\n","    np.save(os.path.join(save_dir, 'sequences_30.npy'), np.array(sequences))\n","    np.save(os.path.join(save_dir, 'labels_30.npy'), np.array(labels))\n","\n","    # Save skill mapping and configuration\n","    metadata = {\n","        'skill_to_id': skill_to_id,\n","        'config': {\n","            'seq_length': config.seq_length,\n","            'sliding_window_step': config.sliding_window_step,\n","            'num_students': config.max_students\n","        },\n","        'dataset_stats': {\n","            'num_sequences': len(sequences),\n","            'sequence_length': len(sequences[0]) if sequences else 0,\n","            'num_skills': len(skill_to_id)\n","        }\n","    }\n","\n","    with open(os.path.join(save_dir, 'metadata_30.json'), 'w') as f:\n","        json.dump(metadata, f, indent=2)\n","\n","    print(f\"Data saved successfully to {save_dir}\")\n","    print(\"Files saved:\")\n","    print(f\"- sequences_30.npy: {os.path.getsize(os.path.join(save_dir, 'sequences_30.npy'))/1024/1024:.2f} MB\")\n","    print(f\"- labels_30.npy: {os.path.getsize(os.path.join(save_dir, 'labels_30.npy'))/1024/1024:.2f} MB\")\n","    print(f\"- metadata_30.json: {os.path.getsize(os.path.join(save_dir, 'metadata_30.json'))/1024:.2f} KB\")\n","\n","# # Save Preprocessed Data:\n","save_preprocessed_data(seq, lab, gen.skill_to_id, gen.config)"]},{"cell_type":"markdown","metadata":{"id":"uDzwvj9jkPsW"},"source":["# **In case already preprocessed, load initial packages and start here**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":53,"status":"ok","timestamp":1740606010569,"user":{"displayName":"Arsh Parmar","userId":"16185547295131340337"},"user_tz":480},"id":"KOP9YxcUi22B","outputId":"8c8d72e3-d832-4116-b83f-abe948c0113a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Data loaded successfully\n","Loaded 94810 sequences\n","Sequence length: 30\n","Number of skills: 90\n","[[ 0  1  2 ... 11 11 14]\n"," [ 1  2  2 ... 11 14 14]\n"," [ 2  2  3 ... 14 14 15]\n"," ...\n"," [10 23 13 ... 35 35 37]\n"," [23 13 11 ... 35 37 37]\n"," [13 11 10 ... 37 37 37]]\n","[[0 1 0 ... 1 1 0]\n"," [1 0 0 ... 1 0 0]\n"," [0 0 1 ... 0 0 1]\n"," ...\n"," [0 1 1 ... 1 1 1]\n"," [1 1 1 ... 1 1 1]\n"," [1 1 0 ... 1 1 1]]\n"]}],"source":["import pickle\n","import json\n","import os\n","from google.colab import drive\n","import numpy as np\n","\n","def load_preprocessed_data(load_dir='/content/drive/MyDrive/DeepKT/preprocessed_data'):\n","    \"\"\"Load preprocessed data from Google Drive\"\"\"\n","    if not os.path.exists('/content/drive'):\n","        drive.mount('/content/drive')\n","\n","    # Load sequences and labels\n","    sequences = np.load(os.path.join(load_dir, 'sequences_30.npy'))\n","    labels = np.load(os.path.join(load_dir, 'labels_30.npy'))\n","\n","    # Load metadata\n","    with open(os.path.join(load_dir, 'metadata_30.json'), 'r') as f:\n","        metadata = json.load(f)\n","\n","    print(\"Data loaded successfully\")\n","    print(f\"Loaded {metadata['dataset_stats']['num_sequences']} sequences\")\n","    print(f\"Sequence length: {metadata['dataset_stats']['sequence_length']}\")\n","    print(f\"Number of skills: {metadata['dataset_stats']['num_skills']}\")\n","\n","    return sequences, labels, metadata\n","\n","# Load in preprocessed data\n","sequences, labels, metadata = load_preprocessed_data()\n","\n","print(sequences[:50])\n","print(labels[:50])"]},{"cell_type":"markdown","metadata":{"id":"5jHKa8nWKRm-"},"source":["# Data Transformation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":100,"status":"ok","timestamp":1740606015714,"user":{"displayName":"Arsh Parmar","userId":"16185547295131340337"},"user_tz":480},"id":"uFtiyMCZKRWE","outputId":"7dfc8983-bffe-44bd-dda3-d911ccede82d"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training inspection:\n","Sequences shape: (64, 30)\n","Labels shape: (64, 30)\n","Sequences dtype: <dtype: 'int32'>\n","Labels dtype: <dtype: 'float32'>\n","\n","Sample sequence (first in batch):\n","Encoded interactions: tf.Tensor(\n","[15 14 14 15 15 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 15 15 15 15\n"," 14 15 15 68 69 54], shape=(30,), dtype=int32)\n","Correctness labels: tf.Tensor(\n","[1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 0. 1. 1. 0. 1. 0.], shape=(30,), dtype=float32)\n","\n","Validation inspection:\n","Sequences shape: (64, 30)\n","Labels shape: (64, 30)\n","Sequences dtype: <dtype: 'int32'>\n","Labels dtype: <dtype: 'float32'>\n","\n","Sample sequence (first in batch):\n","Encoded interactions: tf.Tensor(\n","[117 117 117 117 117 117 116 117 117 116 116 116 154 154 154 155 155 155\n"," 116 116 116 117  54 116 116 116 117 116 116 116], shape=(30,), dtype=int32)\n","Correctness labels: tf.Tensor(\n","[1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0.\n"," 0. 0. 1. 0. 0. 0.], shape=(30,), dtype=float32)\n","\n","Test inspection:\n","Sequences shape: (64, 30)\n","Labels shape: (64, 30)\n","Sequences dtype: <dtype: 'int32'>\n","Labels dtype: <dtype: 'float32'>\n","\n","Sample sequence (first in batch):\n","Encoded interactions: tf.Tensor(\n","[  3   2   3   3   3   3   3   3   2   2   2   3 167  94  95   3 104 104\n"," 104 104 105  75 120 121 120 121 121   3  10 139], shape=(30,), dtype=int32)\n","Correctness labels: tf.Tensor(\n","[1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1.\n"," 0. 1. 1. 1. 0. 1.], shape=(30,), dtype=float32)\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","def prepare_data(sequences, labels, batch_size = 64, train_ratio = 0.7, val_ratio = 0.15):\n","  sequences = sequences.astype(np.int32)\n","  labels = labels.astype(np.float32)\n","\n","  train_sequences, temp_sequences, train_labels, temp_labels = train_test_split(sequences, labels, train_size=train_ratio, random_state=42)\n","\n","  val_ratio_adjusted = val_ratio / (1 - train_ratio)\n","\n","  val_sequences, test_sequences, val_labels, test_labels = train_test_split(temp_sequences, temp_labels, train_size=val_ratio_adjusted, random_state=42)\n","\n","  def create_dataset(sequences, labels, batch_size, training=False):\n","    dataset = tf.data.Dataset.from_tensor_slices((sequences, labels))\n","\n","    if training:\n","      dataset = dataset.shuffle(len(sequences)) # Shuffle tensors\n","\n","    dataset = dataset.batch(batch_size)\n","\n","    if training:\n","      dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE) # Prefetch for optimum training\n","\n","    return dataset\n","\n","  train_dataset = create_dataset(train_sequences, train_labels, batch_size)\n","  val_dataset = create_dataset(val_sequences, val_labels, batch_size)\n","  test_dataset = create_dataset(test_sequences, test_labels, batch_size)\n","\n","  return train_dataset, val_dataset, test_dataset\n","\n","train_dataset, val_dataset, test_dataset = prepare_data(sequences, labels)\n","\n","def inspect_dataset(dataset, name=\"Dataset\"):\n","    \"\"\"Helper function to inspect the prepared datasets\"\"\"\n","    for sequences, labels in dataset.take(1):\n","        print(f\"\\n{name} inspection:\")\n","        print(f\"Sequences shape: {sequences.shape}\")\n","        print(f\"Labels shape: {labels.shape}\")\n","        print(f\"Sequences dtype: {sequences.dtype}\")\n","        print(f\"Labels dtype: {labels.dtype}\")\n","        print(\"\\nSample sequence (first in batch):\")\n","        print(\"Encoded interactions:\", sequences[0])\n","        print(\"Correctness labels:\", labels[0])\n","\n","inspect_dataset(train_dataset, \"Training\")\n","inspect_dataset(val_dataset, \"Validation\")\n","inspect_dataset(test_dataset, \"Test\")"]},{"cell_type":"markdown","metadata":{"id":"J42AT8iQSA98"},"source":["# Transformer Implementation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r0Z-kZIbSGlm"},"outputs":[],"source":["import tensorflow as tf\n","import time\n","import numpy as np\n","from sklearn.metrics import roc_auc_score, accuracy_score\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, MultiHeadAttention, LayerNormalization, Embedding\n","\n","# ================================================\n","# Transformer Block with Causal Masking\n","# ================================================\n","class TransformerBlock(tf.keras.layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super(TransformerBlock, self).__init__()\n","        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = Sequential([\n","            Dense(ff_dim, activation=\"relu\"),\n","            Dense(embed_dim)\n","        ])\n","        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = Dropout(rate)\n","        self.dropout2 = Dropout(rate)\n","\n","    def call(self, inputs, training, mask):\n","        # Pass the mask to the attention layer.\n","        attn_output = self.att(inputs, inputs, inputs, attention_mask=mask)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)\n","\n","# ================================================\n","# Transformer Model\n","# ================================================\n","class TransformerModel(tf.keras.Model):\n","    def __init__(self, num_items, seq_len, embed_dim, num_heads, ff_dim, num_blocks, mlp_units, dropout_rate=0.1):\n","        \"\"\"\n","        Args:\n","          num_items: Size of the vocabulary.\n","          seq_len: Length of the sequences.\n","          embed_dim: Embedding dimension.\n","          num_heads: Number of attention heads.\n","          ff_dim: Feed-forward network hidden layer size.\n","          num_blocks: Number of transformer blocks.\n","          mlp_units: List with the number of units for each MLP Dense layer.\n","          dropout_rate: Dropout rate.\n","        \"\"\"\n","        super(TransformerModel, self).__init__()\n","        self.seq_len = seq_len\n","        self.embedding = Embedding(num_items, embed_dim)\n","        # Positional encoding as an Embedding layer.\n","        self.pos_encoding = Embedding(seq_len, embed_dim)\n","        self.transformer_blocks = [\n","            TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate)\n","            for _ in range(num_blocks)\n","        ]\n","        self.mlp_layers = [Dense(mlp_dim, activation=\"relu\") for mlp_dim in mlp_units]\n","        self.dropout = Dropout(dropout_rate)\n","        self.final_layer = Dense(1)  # No activation here; we apply sigmoid later.\n","\n","    def causal_attention_mask(self, batch_size, seq_len):\n","        \"\"\"\n","        Create a boolean causal mask of shape (batch_size, seq_len, seq_len)\n","        where True indicates allowed (i.e. non-masked) positions.\n","        \"\"\"\n","        # Create a lower-triangular matrix of ones.\n","        lower_triangle = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n","        # Cast to boolean: True means allowed.\n","        mask = tf.cast(lower_triangle, dtype=tf.bool)\n","        # Tile for each sample in the batch.\n","        mask = tf.tile(tf.expand_dims(mask, 0), [batch_size, 1, 1])\n","        return mask\n","\n","    def call(self, inputs, training):\n","        \"\"\"\n","        Args:\n","          inputs: Tensor of shape (batch_size, seq_len) of token IDs.\n","          training: Boolean, for dropout behavior.\n","        Returns:\n","          predictions: Tensor of shape (batch_size, seq_len) with values in [0,1].\n","        \"\"\"\n","        batch_size = tf.shape(inputs)[0]\n","        # Create positional indices.\n","        positions = tf.range(start=0, limit=self.seq_len, delta=1)\n","        positions = tf.expand_dims(positions, 0)           # (1, seq_len)\n","        positions = tf.tile(positions, [batch_size, 1])      # (batch_size, seq_len)\n","\n","        x = self.embedding(inputs)         # (batch_size, seq_len, embed_dim)\n","        pos_enc = self.pos_encoding(positions)\n","        x = x + pos_enc\n","\n","        # Create the causal mask.\n","        mask = self.causal_attention_mask(batch_size, self.seq_len)  # (batch_size, seq_len, seq_len)\n","\n","        # Pass through transformer blocks.\n","        for block in self.transformer_blocks:\n","            x = block(x, training=training, mask=mask)\n","\n","        # Pass through the MLP layers.\n","        for layer in self.mlp_layers:\n","            x = layer(x)\n","            x = self.dropout(x, training=training)\n","\n","        x = self.final_layer(x)  # (batch_size, seq_len, 1)\n","        x = tf.sigmoid(x)        # Map outputs to [0, 1]\n","        return tf.squeeze(x, -1) # (batch_size, seq_len)"]},{"cell_type":"markdown","metadata":{"id":"k8hOxk2FpdwC"},"source":["# Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dtECeYDDpmX2","outputId":"48f70dd7-685a-4113-cc3b-70da046ef183","executionInfo":{"status":"ok","timestamp":1740607847781,"user_tz":480,"elapsed":1222413,"user":{"displayName":"Arsh Parmar","userId":"16185547295131340337"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Epoch 1/50\n","  Batch 50 - Loss: 0.6223\n","  Batch 100 - Loss: 0.6222\n","  Batch 150 - Loss: 0.6363\n","  Batch 200 - Loss: 0.6044\n","  Batch 250 - Loss: 0.6126\n","  Batch 300 - Loss: 0.6095\n","  Batch 350 - Loss: 0.6223\n","  Batch 400 - Loss: 0.6124\n","  Batch 450 - Loss: 0.6185\n","  Batch 500 - Loss: 0.6254\n","  Batch 550 - Loss: 0.6018\n","  Batch 600 - Loss: 0.6026\n","  Batch 650 - Loss: 0.6125\n","  Batch 700 - Loss: 0.6144\n","  Batch 750 - Loss: 0.5880\n","  Batch 800 - Loss: 0.6211\n","  Batch 850 - Loss: 0.5957\n","  Batch 900 - Loss: 0.6128\n","  Batch 950 - Loss: 0.6300\n","  Batch 1000 - Loss: 0.5884\n","Epoch 1 - Average Training Loss: 0.6109\n","  Test Metrics - AUC: 0.6776 | Accuracy: 0.6878\n","  New best model found!\n","Epoch 1 took 50.81s\n","\n","Epoch 2/50\n","  Batch 50 - Loss: 0.5956\n","  Batch 100 - Loss: 0.5953\n","  Batch 150 - Loss: 0.6084\n","  Batch 200 - Loss: 0.5850\n","  Batch 250 - Loss: 0.5905\n","  Batch 300 - Loss: 0.5815\n","  Batch 350 - Loss: 0.6069\n","  Batch 400 - Loss: 0.5930\n","  Batch 450 - Loss: 0.5958\n","  Batch 500 - Loss: 0.5908\n","  Batch 550 - Loss: 0.5729\n","  Batch 600 - Loss: 0.5761\n","  Batch 650 - Loss: 0.5809\n","  Batch 700 - Loss: 0.5952\n","  Batch 750 - Loss: 0.5664\n","  Batch 800 - Loss: 0.5941\n","  Batch 850 - Loss: 0.5648\n","  Batch 900 - Loss: 0.5945\n","  Batch 950 - Loss: 0.6085\n","  Batch 1000 - Loss: 0.5625\n","Epoch 2 - Average Training Loss: 0.5876\n","  Test Metrics - AUC: 0.7083 | Accuracy: 0.6990\n","  New best model found!\n","Epoch 2 took 36.03s\n","\n","Epoch 3/50\n","  Batch 50 - Loss: 0.5761\n","  Batch 100 - Loss: 0.5867\n","  Batch 150 - Loss: 0.5966\n","  Batch 200 - Loss: 0.5629\n","  Batch 250 - Loss: 0.5717\n","  Batch 300 - Loss: 0.5605\n","  Batch 350 - Loss: 0.5973\n","  Batch 400 - Loss: 0.5733\n","  Batch 450 - Loss: 0.5798\n","  Batch 500 - Loss: 0.5751\n","  Batch 550 - Loss: 0.5640\n","  Batch 600 - Loss: 0.5615\n","  Batch 650 - Loss: 0.5664\n","  Batch 700 - Loss: 0.5825\n","  Batch 750 - Loss: 0.5505\n","  Batch 800 - Loss: 0.5813\n","  Batch 850 - Loss: 0.5425\n","  Batch 900 - Loss: 0.5865\n","  Batch 950 - Loss: 0.5935\n","  Batch 1000 - Loss: 0.5451\n","Epoch 3 - Average Training Loss: 0.5732\n","  Test Metrics - AUC: 0.7302 | Accuracy: 0.7098\n","  New best model found!\n","Epoch 3 took 35.91s\n","\n","Epoch 4/50\n","  Batch 50 - Loss: 0.5599\n","  Batch 100 - Loss: 0.5659\n","  Batch 150 - Loss: 0.5771\n","  Batch 200 - Loss: 0.5415\n","  Batch 250 - Loss: 0.5633\n","  Batch 300 - Loss: 0.5473\n","  Batch 350 - Loss: 0.5836\n","  Batch 400 - Loss: 0.5638\n","  Batch 450 - Loss: 0.5716\n","  Batch 500 - Loss: 0.5633\n","  Batch 550 - Loss: 0.5517\n","  Batch 600 - Loss: 0.5507\n","  Batch 650 - Loss: 0.5626\n","  Batch 700 - Loss: 0.5652\n","  Batch 750 - Loss: 0.5361\n","  Batch 800 - Loss: 0.5679\n","  Batch 850 - Loss: 0.5314\n","  Batch 900 - Loss: 0.5726\n","  Batch 950 - Loss: 0.5774\n","  Batch 1000 - Loss: 0.5351\n","Epoch 4 - Average Training Loss: 0.5607\n","  Test Metrics - AUC: 0.7451 | Accuracy: 0.7185\n","  New best model found!\n","Epoch 4 took 35.90s\n","\n","Epoch 5/50\n","  Batch 50 - Loss: 0.5479\n","  Batch 100 - Loss: 0.5508\n","  Batch 150 - Loss: 0.5634\n","  Batch 200 - Loss: 0.5345\n","  Batch 250 - Loss: 0.5432\n","  Batch 300 - Loss: 0.5369\n","  Batch 350 - Loss: 0.5733\n","  Batch 400 - Loss: 0.5459\n","  Batch 450 - Loss: 0.5619\n","  Batch 500 - Loss: 0.5537\n","  Batch 550 - Loss: 0.5433\n","  Batch 600 - Loss: 0.5434\n","  Batch 650 - Loss: 0.5514\n","  Batch 700 - Loss: 0.5505\n","  Batch 750 - Loss: 0.5263\n","  Batch 800 - Loss: 0.5555\n","  Batch 850 - Loss: 0.5177\n","  Batch 900 - Loss: 0.5674\n","  Batch 950 - Loss: 0.5767\n","  Batch 1000 - Loss: 0.5287\n","Epoch 5 - Average Training Loss: 0.5489\n","  Test Metrics - AUC: 0.7591 | Accuracy: 0.7265\n","  New best model found!\n","Epoch 5 took 36.33s\n","\n","Epoch 6/50\n","  Batch 50 - Loss: 0.5380\n","  Batch 100 - Loss: 0.5479\n","  Batch 150 - Loss: 0.5544\n","  Batch 200 - Loss: 0.5355\n","  Batch 250 - Loss: 0.5352\n","  Batch 300 - Loss: 0.5323\n","  Batch 350 - Loss: 0.5620\n","  Batch 400 - Loss: 0.5400\n","  Batch 450 - Loss: 0.5511\n","  Batch 500 - Loss: 0.5483\n","  Batch 550 - Loss: 0.5319\n","  Batch 600 - Loss: 0.5318\n","  Batch 650 - Loss: 0.5478\n","  Batch 700 - Loss: 0.5477\n","  Batch 750 - Loss: 0.5023\n","  Batch 800 - Loss: 0.5427\n","  Batch 850 - Loss: 0.5030\n","  Batch 900 - Loss: 0.5566\n","  Batch 950 - Loss: 0.5668\n","  Batch 1000 - Loss: 0.5028\n","Epoch 6 - Average Training Loss: 0.5373\n","  Test Metrics - AUC: 0.7710 | Accuracy: 0.7353\n","  New best model found!\n","Epoch 6 took 36.46s\n","\n","Epoch 7/50\n","  Batch 50 - Loss: 0.5303\n","  Batch 100 - Loss: 0.5311\n","  Batch 150 - Loss: 0.5466\n","  Batch 200 - Loss: 0.5133\n","  Batch 250 - Loss: 0.5244\n","  Batch 300 - Loss: 0.5199\n","  Batch 350 - Loss: 0.5467\n","  Batch 400 - Loss: 0.5308\n","  Batch 450 - Loss: 0.5485\n","  Batch 500 - Loss: 0.5303\n","  Batch 550 - Loss: 0.5175\n","  Batch 600 - Loss: 0.5204\n","  Batch 650 - Loss: 0.5448\n","  Batch 700 - Loss: 0.5347\n","  Batch 750 - Loss: 0.4944\n","  Batch 800 - Loss: 0.5342\n","  Batch 850 - Loss: 0.4951\n","  Batch 900 - Loss: 0.5464\n","  Batch 950 - Loss: 0.5590\n","  Batch 1000 - Loss: 0.5011\n","Epoch 7 - Average Training Loss: 0.5256\n","  Test Metrics - AUC: 0.7860 | Accuracy: 0.7441\n","  New best model found!\n","Epoch 7 took 36.39s\n","\n","Epoch 8/50\n","  Batch 50 - Loss: 0.5262\n","  Batch 100 - Loss: 0.5139\n","  Batch 150 - Loss: 0.5257\n","  Batch 200 - Loss: 0.5049\n","  Batch 250 - Loss: 0.5200\n","  Batch 300 - Loss: 0.5028\n","  Batch 350 - Loss: 0.5337\n","  Batch 400 - Loss: 0.5119\n","  Batch 450 - Loss: 0.5244\n","  Batch 500 - Loss: 0.5195\n","  Batch 550 - Loss: 0.4970\n","  Batch 600 - Loss: 0.5085\n","  Batch 650 - Loss: 0.5303\n","  Batch 700 - Loss: 0.5113\n","  Batch 750 - Loss: 0.4734\n","  Batch 800 - Loss: 0.5210\n","  Batch 850 - Loss: 0.4819\n","  Batch 900 - Loss: 0.5453\n","  Batch 950 - Loss: 0.5556\n","  Batch 1000 - Loss: 0.4860\n","Epoch 8 - Average Training Loss: 0.5152\n","  Test Metrics - AUC: 0.7969 | Accuracy: 0.7506\n","  New best model found!\n","Epoch 8 took 36.51s\n","\n","Epoch 9/50\n","  Batch 50 - Loss: 0.5142\n","  Batch 100 - Loss: 0.5124\n","  Batch 150 - Loss: 0.5149\n","  Batch 200 - Loss: 0.5020\n","  Batch 250 - Loss: 0.5089\n","  Batch 300 - Loss: 0.5047\n","  Batch 350 - Loss: 0.5218\n","  Batch 400 - Loss: 0.5030\n","  Batch 450 - Loss: 0.5196\n","  Batch 500 - Loss: 0.5045\n","  Batch 550 - Loss: 0.4876\n","  Batch 600 - Loss: 0.5063\n","  Batch 650 - Loss: 0.5151\n","  Batch 700 - Loss: 0.5067\n","  Batch 750 - Loss: 0.4621\n","  Batch 800 - Loss: 0.5123\n","  Batch 850 - Loss: 0.4679\n","  Batch 900 - Loss: 0.5279\n","  Batch 950 - Loss: 0.5425\n","  Batch 1000 - Loss: 0.4871\n","Epoch 9 - Average Training Loss: 0.5052\n","  Test Metrics - AUC: 0.8060 | Accuracy: 0.7574\n","  New best model found!\n","Epoch 9 took 36.45s\n","\n","Epoch 10/50\n","  Batch 50 - Loss: 0.5030\n","  Batch 100 - Loss: 0.5070\n","  Batch 150 - Loss: 0.5055\n","  Batch 200 - Loss: 0.4902\n","  Batch 250 - Loss: 0.4969\n","  Batch 300 - Loss: 0.4931\n","  Batch 350 - Loss: 0.5143\n","  Batch 400 - Loss: 0.4948\n","  Batch 450 - Loss: 0.5063\n","  Batch 500 - Loss: 0.5002\n","  Batch 550 - Loss: 0.4829\n","  Batch 600 - Loss: 0.4917\n","  Batch 650 - Loss: 0.5019\n","  Batch 700 - Loss: 0.4975\n","  Batch 750 - Loss: 0.4607\n","  Batch 800 - Loss: 0.4969\n","  Batch 850 - Loss: 0.4599\n","  Batch 900 - Loss: 0.5239\n","  Batch 950 - Loss: 0.5204\n","  Batch 1000 - Loss: 0.4700\n","Epoch 10 - Average Training Loss: 0.4962\n","  Test Metrics - AUC: 0.8135 | Accuracy: 0.7617\n","  New best model found!\n","Epoch 10 took 36.28s\n","\n","Epoch 11/50\n","  Batch 50 - Loss: 0.4976\n","  Batch 100 - Loss: 0.4907\n","  Batch 150 - Loss: 0.4838\n","  Batch 200 - Loss: 0.4922\n","  Batch 250 - Loss: 0.5032\n","  Batch 300 - Loss: 0.4907\n","  Batch 350 - Loss: 0.5107\n","  Batch 400 - Loss: 0.4956\n","  Batch 450 - Loss: 0.4972\n","  Batch 500 - Loss: 0.4878\n","  Batch 550 - Loss: 0.4756\n","  Batch 600 - Loss: 0.5045\n","  Batch 650 - Loss: 0.4999\n","  Batch 700 - Loss: 0.4944\n","  Batch 750 - Loss: 0.4481\n","  Batch 800 - Loss: 0.4941\n","  Batch 850 - Loss: 0.4553\n","  Batch 900 - Loss: 0.5133\n","  Batch 950 - Loss: 0.5117\n","  Batch 1000 - Loss: 0.4747\n","Epoch 11 - Average Training Loss: 0.4878\n","  Test Metrics - AUC: 0.8226 | Accuracy: 0.7683\n","  New best model found!\n","Epoch 11 took 35.97s\n","\n","Epoch 12/50\n","  Batch 50 - Loss: 0.4829\n","  Batch 100 - Loss: 0.4678\n","  Batch 150 - Loss: 0.4669\n","  Batch 200 - Loss: 0.4692\n","  Batch 250 - Loss: 0.4850\n","  Batch 300 - Loss: 0.4854\n","  Batch 350 - Loss: 0.4892\n","  Batch 400 - Loss: 0.4830\n","  Batch 450 - Loss: 0.4845\n","  Batch 500 - Loss: 0.4749\n","  Batch 550 - Loss: 0.4614\n","  Batch 600 - Loss: 0.4858\n","  Batch 650 - Loss: 0.4849\n","  Batch 700 - Loss: 0.4826\n","  Batch 750 - Loss: 0.4353\n","  Batch 800 - Loss: 0.4860\n","  Batch 850 - Loss: 0.4359\n","  Batch 900 - Loss: 0.4941\n","  Batch 950 - Loss: 0.5094\n","  Batch 1000 - Loss: 0.4652\n","Epoch 12 - Average Training Loss: 0.4803\n","  Test Metrics - AUC: 0.8288 | Accuracy: 0.7726\n","  New best model found!\n","Epoch 12 took 35.95s\n","\n","Epoch 13/50\n","  Batch 50 - Loss: 0.4668\n","  Batch 100 - Loss: 0.4607\n","  Batch 150 - Loss: 0.4706\n","  Batch 200 - Loss: 0.4665\n","  Batch 250 - Loss: 0.4764\n","  Batch 300 - Loss: 0.4718\n","  Batch 350 - Loss: 0.4916\n","  Batch 400 - Loss: 0.4741\n","  Batch 450 - Loss: 0.4806\n","  Batch 500 - Loss: 0.4724\n","  Batch 550 - Loss: 0.4672\n","  Batch 600 - Loss: 0.4750\n","  Batch 650 - Loss: 0.4817\n","  Batch 700 - Loss: 0.4790\n","  Batch 750 - Loss: 0.4220\n","  Batch 800 - Loss: 0.4777\n","  Batch 850 - Loss: 0.4315\n","  Batch 900 - Loss: 0.5023\n","  Batch 950 - Loss: 0.4984\n","  Batch 1000 - Loss: 0.4663\n","Epoch 13 - Average Training Loss: 0.4730\n","  Test Metrics - AUC: 0.8340 | Accuracy: 0.7766\n","  New best model found!\n","Epoch 13 took 35.71s\n","\n","Epoch 14/50\n","  Batch 50 - Loss: 0.4632\n","  Batch 100 - Loss: 0.4618\n","  Batch 150 - Loss: 0.4620\n","  Batch 200 - Loss: 0.4587\n","  Batch 250 - Loss: 0.4701\n","  Batch 300 - Loss: 0.4782\n","  Batch 350 - Loss: 0.4667\n","  Batch 400 - Loss: 0.4560\n","  Batch 450 - Loss: 0.4640\n","  Batch 500 - Loss: 0.4658\n","  Batch 550 - Loss: 0.4422\n","  Batch 600 - Loss: 0.4674\n","  Batch 650 - Loss: 0.4693\n","  Batch 700 - Loss: 0.4526\n","  Batch 750 - Loss: 0.4350\n","  Batch 800 - Loss: 0.4726\n","  Batch 850 - Loss: 0.4404\n","  Batch 900 - Loss: 0.4988\n","  Batch 950 - Loss: 0.5037\n","  Batch 1000 - Loss: 0.4483\n","Epoch 14 - Average Training Loss: 0.4663\n","  Test Metrics - AUC: 0.8409 | Accuracy: 0.7804\n","  New best model found!\n","Epoch 14 took 35.81s\n","\n","Epoch 15/50\n","  Batch 50 - Loss: 0.4628\n","  Batch 100 - Loss: 0.4506\n","  Batch 150 - Loss: 0.4560\n","  Batch 200 - Loss: 0.4502\n","  Batch 250 - Loss: 0.4589\n","  Batch 300 - Loss: 0.4758\n","  Batch 350 - Loss: 0.4637\n","  Batch 400 - Loss: 0.4641\n","  Batch 450 - Loss: 0.4690\n","  Batch 500 - Loss: 0.4597\n","  Batch 550 - Loss: 0.4557\n","  Batch 600 - Loss: 0.4599\n","  Batch 650 - Loss: 0.4666\n","  Batch 700 - Loss: 0.4556\n","  Batch 750 - Loss: 0.4178\n","  Batch 800 - Loss: 0.4810\n","  Batch 850 - Loss: 0.4253\n","  Batch 900 - Loss: 0.4862\n","  Batch 950 - Loss: 0.4793\n","  Batch 1000 - Loss: 0.4406\n","Epoch 15 - Average Training Loss: 0.4609\n","  Test Metrics - AUC: 0.8427 | Accuracy: 0.7822\n","  New best model found!\n","Epoch 15 took 35.84s\n","\n","Epoch 16/50\n","  Batch 50 - Loss: 0.4540\n","  Batch 100 - Loss: 0.4515\n","  Batch 150 - Loss: 0.4343\n","  Batch 200 - Loss: 0.4351\n","  Batch 250 - Loss: 0.4642\n","  Batch 300 - Loss: 0.4584\n","  Batch 350 - Loss: 0.4634\n","  Batch 400 - Loss: 0.4515\n","  Batch 450 - Loss: 0.4603\n","  Batch 500 - Loss: 0.4572\n","  Batch 550 - Loss: 0.4301\n","  Batch 600 - Loss: 0.4550\n","  Batch 650 - Loss: 0.4646\n","  Batch 700 - Loss: 0.4594\n","  Batch 750 - Loss: 0.4238\n","  Batch 800 - Loss: 0.4520\n","  Batch 850 - Loss: 0.4303\n","  Batch 900 - Loss: 0.4877\n","  Batch 950 - Loss: 0.4883\n","  Batch 1000 - Loss: 0.4440\n","Epoch 16 - Average Training Loss: 0.4550\n","  Test Metrics - AUC: 0.8488 | Accuracy: 0.7868\n","  New best model found!\n","Epoch 16 took 35.92s\n","\n","Epoch 17/50\n","  Batch 50 - Loss: 0.4562\n","  Batch 100 - Loss: 0.4438\n","  Batch 150 - Loss: 0.4437\n","  Batch 200 - Loss: 0.4272\n","  Batch 250 - Loss: 0.4418\n","  Batch 300 - Loss: 0.4632\n","  Batch 350 - Loss: 0.4660\n","  Batch 400 - Loss: 0.4507\n","  Batch 450 - Loss: 0.4612\n","  Batch 500 - Loss: 0.4466\n","  Batch 550 - Loss: 0.4287\n","  Batch 600 - Loss: 0.4500\n","  Batch 650 - Loss: 0.4652\n","  Batch 700 - Loss: 0.4548\n","  Batch 750 - Loss: 0.4085\n","  Batch 800 - Loss: 0.4543\n","  Batch 850 - Loss: 0.4082\n","  Batch 900 - Loss: 0.4757\n","  Batch 950 - Loss: 0.4767\n","  Batch 1000 - Loss: 0.4419\n","Epoch 17 - Average Training Loss: 0.4500\n","  Test Metrics - AUC: 0.8535 | Accuracy: 0.7898\n","  New best model found!\n","Epoch 17 took 35.85s\n","\n","Epoch 18/50\n","  Batch 50 - Loss: 0.4437\n","  Batch 100 - Loss: 0.4391\n","  Batch 150 - Loss: 0.4426\n","  Batch 200 - Loss: 0.4376\n","  Batch 250 - Loss: 0.4343\n","  Batch 300 - Loss: 0.4417\n","  Batch 350 - Loss: 0.4577\n","  Batch 400 - Loss: 0.4515\n","  Batch 450 - Loss: 0.4569\n","  Batch 500 - Loss: 0.4417\n","  Batch 550 - Loss: 0.4270\n","  Batch 600 - Loss: 0.4652\n","  Batch 650 - Loss: 0.4500\n","  Batch 700 - Loss: 0.4476\n","  Batch 750 - Loss: 0.4105\n","  Batch 800 - Loss: 0.4515\n","  Batch 850 - Loss: 0.4146\n","  Batch 900 - Loss: 0.4648\n","  Batch 950 - Loss: 0.4753\n","  Batch 1000 - Loss: 0.4236\n","Epoch 18 - Average Training Loss: 0.4457\n","  Test Metrics - AUC: 0.8561 | Accuracy: 0.7931\n","  New best model found!\n","Epoch 18 took 35.93s\n","\n","Epoch 19/50\n","  Batch 50 - Loss: 0.4347\n","  Batch 100 - Loss: 0.4332\n","  Batch 150 - Loss: 0.4260\n","  Batch 200 - Loss: 0.4315\n","  Batch 250 - Loss: 0.4478\n","  Batch 300 - Loss: 0.4499\n","  Batch 350 - Loss: 0.4549\n","  Batch 400 - Loss: 0.4464\n","  Batch 450 - Loss: 0.4557\n","  Batch 500 - Loss: 0.4438\n","  Batch 550 - Loss: 0.4356\n","  Batch 600 - Loss: 0.4499\n","  Batch 650 - Loss: 0.4634\n","  Batch 700 - Loss: 0.4300\n","  Batch 750 - Loss: 0.3960\n","  Batch 800 - Loss: 0.4434\n","  Batch 850 - Loss: 0.4195\n","  Batch 900 - Loss: 0.4652\n","  Batch 950 - Loss: 0.4745\n","  Batch 1000 - Loss: 0.4291\n","Epoch 19 - Average Training Loss: 0.4412\n","  Test Metrics - AUC: 0.8607 | Accuracy: 0.7958\n","  New best model found!\n","Epoch 19 took 35.86s\n","\n","Epoch 20/50\n","  Batch 50 - Loss: 0.4306\n","  Batch 100 - Loss: 0.4427\n","  Batch 150 - Loss: 0.4294\n","  Batch 200 - Loss: 0.4192\n","  Batch 250 - Loss: 0.4461\n","  Batch 300 - Loss: 0.4467\n","  Batch 350 - Loss: 0.4637\n","  Batch 400 - Loss: 0.4416\n","  Batch 450 - Loss: 0.4419\n","  Batch 500 - Loss: 0.4423\n","  Batch 550 - Loss: 0.4257\n","  Batch 600 - Loss: 0.4433\n","  Batch 650 - Loss: 0.4448\n","  Batch 700 - Loss: 0.4405\n","  Batch 750 - Loss: 0.4009\n","  Batch 800 - Loss: 0.4370\n","  Batch 850 - Loss: 0.4178\n","  Batch 900 - Loss: 0.4707\n","  Batch 950 - Loss: 0.4554\n","  Batch 1000 - Loss: 0.4262\n","Epoch 20 - Average Training Loss: 0.4373\n","  Test Metrics - AUC: 0.8642 | Accuracy: 0.7986\n","  New best model found!\n","Epoch 20 took 35.86s\n","\n","Epoch 21/50\n","  Batch 50 - Loss: 0.4456\n","  Batch 100 - Loss: 0.4325\n","  Batch 150 - Loss: 0.4101\n","  Batch 200 - Loss: 0.4302\n","  Batch 250 - Loss: 0.4323\n","  Batch 300 - Loss: 0.4508\n","  Batch 350 - Loss: 0.4330\n","  Batch 400 - Loss: 0.4347\n","  Batch 450 - Loss: 0.4377\n","  Batch 500 - Loss: 0.4362\n","  Batch 550 - Loss: 0.4211\n","  Batch 600 - Loss: 0.4569\n","  Batch 650 - Loss: 0.4461\n","  Batch 700 - Loss: 0.4468\n","  Batch 750 - Loss: 0.3914\n","  Batch 800 - Loss: 0.4350\n","  Batch 850 - Loss: 0.4138\n","  Batch 900 - Loss: 0.4533\n","  Batch 950 - Loss: 0.4725\n","  Batch 1000 - Loss: 0.4243\n","Epoch 21 - Average Training Loss: 0.4335\n","  Test Metrics - AUC: 0.8666 | Accuracy: 0.7995\n","  New best model found!\n","Epoch 21 took 35.75s\n","\n","Epoch 22/50\n","  Batch 50 - Loss: 0.4281\n","  Batch 100 - Loss: 0.4172\n","  Batch 150 - Loss: 0.4199\n","  Batch 200 - Loss: 0.4196\n","  Batch 250 - Loss: 0.4363\n","  Batch 300 - Loss: 0.4492\n","  Batch 350 - Loss: 0.4424\n","  Batch 400 - Loss: 0.4306\n","  Batch 450 - Loss: 0.4458\n","  Batch 500 - Loss: 0.4435\n","  Batch 550 - Loss: 0.4249\n","  Batch 600 - Loss: 0.4330\n","  Batch 650 - Loss: 0.4216\n","  Batch 700 - Loss: 0.4197\n","  Batch 750 - Loss: 0.4084\n","  Batch 800 - Loss: 0.4239\n","  Batch 850 - Loss: 0.4149\n","  Batch 900 - Loss: 0.4403\n","  Batch 950 - Loss: 0.4497\n","  Batch 1000 - Loss: 0.4211\n","Epoch 22 - Average Training Loss: 0.4298\n","  Test Metrics - AUC: 0.8691 | Accuracy: 0.8016\n","  New best model found!\n","Epoch 22 took 35.85s\n","\n","Epoch 23/50\n","  Batch 50 - Loss: 0.4298\n","  Batch 100 - Loss: 0.4291\n","  Batch 150 - Loss: 0.4121\n","  Batch 200 - Loss: 0.4285\n","  Batch 250 - Loss: 0.4296\n","  Batch 300 - Loss: 0.4459\n","  Batch 350 - Loss: 0.4406\n","  Batch 400 - Loss: 0.4289\n","  Batch 450 - Loss: 0.4367\n","  Batch 500 - Loss: 0.4194\n","  Batch 550 - Loss: 0.4250\n","  Batch 600 - Loss: 0.4418\n","  Batch 650 - Loss: 0.4228\n","  Batch 700 - Loss: 0.4243\n","  Batch 750 - Loss: 0.3857\n","  Batch 800 - Loss: 0.4387\n","  Batch 850 - Loss: 0.3998\n","  Batch 900 - Loss: 0.4379\n","  Batch 950 - Loss: 0.4564\n","  Batch 1000 - Loss: 0.4189\n","Epoch 23 - Average Training Loss: 0.4263\n","  Test Metrics - AUC: 0.8714 | Accuracy: 0.8035\n","  New best model found!\n","Epoch 23 took 35.90s\n","\n","Epoch 24/50\n","  Batch 50 - Loss: 0.4205\n","  Batch 100 - Loss: 0.4201\n","  Batch 150 - Loss: 0.4084\n","  Batch 200 - Loss: 0.4049\n","  Batch 250 - Loss: 0.4250\n","  Batch 300 - Loss: 0.4282\n","  Batch 350 - Loss: 0.4323\n","  Batch 400 - Loss: 0.4213\n","  Batch 450 - Loss: 0.4270\n","  Batch 500 - Loss: 0.4154\n","  Batch 550 - Loss: 0.4078\n","  Batch 600 - Loss: 0.4351\n","  Batch 650 - Loss: 0.4125\n","  Batch 700 - Loss: 0.4180\n","  Batch 750 - Loss: 0.3845\n","  Batch 800 - Loss: 0.4235\n","  Batch 850 - Loss: 0.3941\n","  Batch 900 - Loss: 0.4333\n","  Batch 950 - Loss: 0.4540\n","  Batch 1000 - Loss: 0.4210\n","Epoch 24 - Average Training Loss: 0.4233\n","  Test Metrics - AUC: 0.8740 | Accuracy: 0.8059\n","  New best model found!\n","Epoch 24 took 35.90s\n","\n","Epoch 25/50\n","  Batch 50 - Loss: 0.4337\n","  Batch 100 - Loss: 0.4217\n","  Batch 150 - Loss: 0.4062\n","  Batch 200 - Loss: 0.4185\n","  Batch 250 - Loss: 0.4264\n","  Batch 300 - Loss: 0.4364\n","  Batch 350 - Loss: 0.4306\n","  Batch 400 - Loss: 0.4254\n","  Batch 450 - Loss: 0.4324\n","  Batch 500 - Loss: 0.4144\n","  Batch 550 - Loss: 0.4174\n","  Batch 600 - Loss: 0.4244\n","  Batch 650 - Loss: 0.4292\n","  Batch 700 - Loss: 0.4149\n","  Batch 750 - Loss: 0.3889\n","  Batch 800 - Loss: 0.4286\n","  Batch 850 - Loss: 0.4083\n","  Batch 900 - Loss: 0.4379\n","  Batch 950 - Loss: 0.4443\n","  Batch 1000 - Loss: 0.4060\n","Epoch 25 - Average Training Loss: 0.4204\n","  Test Metrics - AUC: 0.8775 | Accuracy: 0.8097\n","  New best model found!\n","Epoch 25 took 35.87s\n","\n","Epoch 26/50\n","  Batch 50 - Loss: 0.4165\n","  Batch 100 - Loss: 0.4034\n","  Batch 150 - Loss: 0.4094\n","  Batch 200 - Loss: 0.3913\n","  Batch 250 - Loss: 0.4288\n","  Batch 300 - Loss: 0.4403\n","  Batch 350 - Loss: 0.4302\n","  Batch 400 - Loss: 0.4207\n","  Batch 450 - Loss: 0.4213\n","  Batch 500 - Loss: 0.4183\n","  Batch 550 - Loss: 0.4119\n","  Batch 600 - Loss: 0.4336\n","  Batch 650 - Loss: 0.4063\n","  Batch 700 - Loss: 0.4302\n","  Batch 750 - Loss: 0.3942\n","  Batch 800 - Loss: 0.4273\n","  Batch 850 - Loss: 0.3829\n","  Batch 900 - Loss: 0.4371\n","  Batch 950 - Loss: 0.4393\n","  Batch 1000 - Loss: 0.4124\n","Epoch 26 - Average Training Loss: 0.4173\n","  Test Metrics - AUC: 0.8791 | Accuracy: 0.8103\n","  New best model found!\n","Epoch 26 took 35.80s\n","\n","Epoch 27/50\n","  Batch 50 - Loss: 0.4247\n","  Batch 100 - Loss: 0.4128\n","  Batch 150 - Loss: 0.3964\n","  Batch 200 - Loss: 0.3975\n","  Batch 250 - Loss: 0.4169\n","  Batch 300 - Loss: 0.4250\n","  Batch 350 - Loss: 0.4201\n","  Batch 400 - Loss: 0.4148\n","  Batch 450 - Loss: 0.4168\n","  Batch 500 - Loss: 0.4097\n","  Batch 550 - Loss: 0.4003\n","  Batch 600 - Loss: 0.4189\n","  Batch 650 - Loss: 0.4186\n","  Batch 700 - Loss: 0.4224\n","  Batch 750 - Loss: 0.3726\n","  Batch 800 - Loss: 0.4248\n","  Batch 850 - Loss: 0.3841\n","  Batch 900 - Loss: 0.4314\n","  Batch 950 - Loss: 0.4360\n","  Batch 1000 - Loss: 0.4112\n","Epoch 27 - Average Training Loss: 0.4147\n","  Test Metrics - AUC: 0.8788 | Accuracy: 0.8094\n","Epoch 27 took 35.87s\n","\n","Epoch 28/50\n","  Batch 50 - Loss: 0.4130\n","  Batch 100 - Loss: 0.4128\n","  Batch 150 - Loss: 0.3972\n","  Batch 200 - Loss: 0.4056\n","  Batch 250 - Loss: 0.4109\n","  Batch 300 - Loss: 0.4277\n","  Batch 350 - Loss: 0.4212\n","  Batch 400 - Loss: 0.4147\n","  Batch 450 - Loss: 0.4140\n","  Batch 500 - Loss: 0.4133\n","  Batch 550 - Loss: 0.4076\n","  Batch 600 - Loss: 0.4216\n","  Batch 650 - Loss: 0.4173\n","  Batch 700 - Loss: 0.4120\n","  Batch 750 - Loss: 0.3823\n","  Batch 800 - Loss: 0.4139\n","  Batch 850 - Loss: 0.3864\n","  Batch 900 - Loss: 0.4313\n","  Batch 950 - Loss: 0.4397\n","  Batch 1000 - Loss: 0.4049\n","Epoch 28 - Average Training Loss: 0.4122\n","  Test Metrics - AUC: 0.8826 | Accuracy: 0.8121\n","  New best model found!\n","Epoch 28 took 35.80s\n","\n","Epoch 29/50\n","  Batch 50 - Loss: 0.4055\n","  Batch 100 - Loss: 0.4057\n","  Batch 150 - Loss: 0.3835\n","  Batch 200 - Loss: 0.3944\n","  Batch 250 - Loss: 0.4156\n","  Batch 300 - Loss: 0.4282\n","  Batch 350 - Loss: 0.4187\n","  Batch 400 - Loss: 0.3958\n","  Batch 450 - Loss: 0.4120\n","  Batch 500 - Loss: 0.4095\n","  Batch 550 - Loss: 0.3959\n","  Batch 600 - Loss: 0.4172\n","  Batch 650 - Loss: 0.4027\n","  Batch 700 - Loss: 0.4064\n","  Batch 750 - Loss: 0.3782\n","  Batch 800 - Loss: 0.4265\n","  Batch 850 - Loss: 0.3849\n","  Batch 900 - Loss: 0.4114\n","  Batch 950 - Loss: 0.4409\n","  Batch 1000 - Loss: 0.4059\n","Epoch 29 - Average Training Loss: 0.4097\n","  Test Metrics - AUC: 0.8848 | Accuracy: 0.8150\n","  New best model found!\n","Epoch 29 took 35.89s\n","\n","Epoch 30/50\n","  Batch 50 - Loss: 0.4136\n","  Batch 100 - Loss: 0.3889\n","  Batch 150 - Loss: 0.4014\n","  Batch 200 - Loss: 0.3884\n","  Batch 250 - Loss: 0.4034\n","  Batch 300 - Loss: 0.4264\n","  Batch 350 - Loss: 0.4189\n","  Batch 400 - Loss: 0.4133\n","  Batch 450 - Loss: 0.4096\n","  Batch 500 - Loss: 0.4074\n","  Batch 550 - Loss: 0.3840\n","  Batch 600 - Loss: 0.4254\n","  Batch 650 - Loss: 0.4186\n","  Batch 700 - Loss: 0.4169\n","  Batch 750 - Loss: 0.3639\n","  Batch 800 - Loss: 0.4178\n","  Batch 850 - Loss: 0.3822\n","  Batch 900 - Loss: 0.4195\n","  Batch 950 - Loss: 0.4402\n","  Batch 1000 - Loss: 0.4116\n","Epoch 30 - Average Training Loss: 0.4079\n","  Test Metrics - AUC: 0.8846 | Accuracy: 0.8143\n","Epoch 30 took 36.00s\n","\n","Epoch 31/50\n","  Batch 50 - Loss: 0.4208\n","  Batch 100 - Loss: 0.4019\n","  Batch 150 - Loss: 0.3942\n","  Batch 200 - Loss: 0.3822\n","  Batch 250 - Loss: 0.4104\n","  Batch 300 - Loss: 0.4226\n","  Batch 350 - Loss: 0.4166\n","  Batch 400 - Loss: 0.4080\n","  Batch 450 - Loss: 0.4277\n","  Batch 500 - Loss: 0.4037\n","  Batch 550 - Loss: 0.3937\n","  Batch 600 - Loss: 0.4164\n","  Batch 650 - Loss: 0.4013\n","  Batch 700 - Loss: 0.4030\n","  Batch 750 - Loss: 0.3719\n","  Batch 800 - Loss: 0.4174\n","  Batch 850 - Loss: 0.3846\n","  Batch 900 - Loss: 0.4177\n","  Batch 950 - Loss: 0.4297\n","  Batch 1000 - Loss: 0.3979\n","Epoch 31 - Average Training Loss: 0.4053\n","  Test Metrics - AUC: 0.8852 | Accuracy: 0.8148\n","  New best model found!\n","Epoch 31 took 35.93s\n","\n","Epoch 32/50\n","  Batch 50 - Loss: 0.4091\n","  Batch 100 - Loss: 0.4029\n","  Batch 150 - Loss: 0.3892\n","  Batch 200 - Loss: 0.3933\n","  Batch 250 - Loss: 0.4010\n","  Batch 300 - Loss: 0.4205\n","  Batch 350 - Loss: 0.4142\n","  Batch 400 - Loss: 0.4012\n","  Batch 450 - Loss: 0.4083\n","  Batch 500 - Loss: 0.4013\n","  Batch 550 - Loss: 0.4093\n","  Batch 600 - Loss: 0.4250\n","  Batch 650 - Loss: 0.4106\n","  Batch 700 - Loss: 0.3956\n","  Batch 750 - Loss: 0.3737\n","  Batch 800 - Loss: 0.4097\n","  Batch 850 - Loss: 0.3720\n","  Batch 900 - Loss: 0.4210\n","  Batch 950 - Loss: 0.4209\n","  Batch 1000 - Loss: 0.3980\n","Epoch 32 - Average Training Loss: 0.4035\n","  Test Metrics - AUC: 0.8884 | Accuracy: 0.8176\n","  New best model found!\n","Epoch 32 took 35.91s\n","\n","Epoch 33/50\n","  Batch 50 - Loss: 0.4092\n","  Batch 100 - Loss: 0.3874\n","  Batch 150 - Loss: 0.3741\n","  Batch 200 - Loss: 0.3827\n","  Batch 250 - Loss: 0.4046\n","  Batch 300 - Loss: 0.4126\n","  Batch 350 - Loss: 0.4030\n","  Batch 400 - Loss: 0.3886\n","  Batch 450 - Loss: 0.4133\n","  Batch 500 - Loss: 0.4013\n","  Batch 550 - Loss: 0.4078\n","  Batch 600 - Loss: 0.4073\n","  Batch 650 - Loss: 0.3836\n","  Batch 700 - Loss: 0.4037\n","  Batch 750 - Loss: 0.3592\n","  Batch 800 - Loss: 0.4075\n","  Batch 850 - Loss: 0.3748\n","  Batch 900 - Loss: 0.4184\n","  Batch 950 - Loss: 0.4327\n","  Batch 1000 - Loss: 0.4002\n","Epoch 33 - Average Training Loss: 0.4010\n","  Test Metrics - AUC: 0.8893 | Accuracy: 0.8190\n","  New best model found!\n","Epoch 33 took 35.83s\n","\n","Epoch 34/50\n","  Batch 50 - Loss: 0.4069\n","  Batch 100 - Loss: 0.3912\n","  Batch 150 - Loss: 0.3792\n","  Batch 200 - Loss: 0.3903\n","  Batch 250 - Loss: 0.4024\n","  Batch 300 - Loss: 0.4244\n","  Batch 350 - Loss: 0.4100\n","  Batch 400 - Loss: 0.4012\n","  Batch 450 - Loss: 0.3921\n","  Batch 500 - Loss: 0.4086\n","  Batch 550 - Loss: 0.3887\n","  Batch 600 - Loss: 0.4159\n","  Batch 650 - Loss: 0.4109\n","  Batch 700 - Loss: 0.3905\n","  Batch 750 - Loss: 0.3626\n","  Batch 800 - Loss: 0.4049\n","  Batch 850 - Loss: 0.3791\n","  Batch 900 - Loss: 0.4059\n","  Batch 950 - Loss: 0.4157\n","  Batch 1000 - Loss: 0.3872\n","Epoch 34 - Average Training Loss: 0.3992\n","  Test Metrics - AUC: 0.8912 | Accuracy: 0.8207\n","  New best model found!\n","Epoch 34 took 35.85s\n","\n","Epoch 35/50\n","  Batch 50 - Loss: 0.3969\n","  Batch 100 - Loss: 0.4015\n","  Batch 150 - Loss: 0.3820\n","  Batch 200 - Loss: 0.3926\n","  Batch 250 - Loss: 0.4049\n","  Batch 300 - Loss: 0.3989\n","  Batch 350 - Loss: 0.3983\n","  Batch 400 - Loss: 0.3958\n","  Batch 450 - Loss: 0.4036\n","  Batch 500 - Loss: 0.3837\n","  Batch 550 - Loss: 0.3948\n","  Batch 600 - Loss: 0.4194\n","  Batch 650 - Loss: 0.4111\n","  Batch 700 - Loss: 0.3892\n","  Batch 750 - Loss: 0.3790\n","  Batch 800 - Loss: 0.4060\n","  Batch 850 - Loss: 0.3765\n","  Batch 900 - Loss: 0.4081\n","  Batch 950 - Loss: 0.4268\n","  Batch 1000 - Loss: 0.3790\n","Epoch 35 - Average Training Loss: 0.3977\n","  Test Metrics - AUC: 0.8916 | Accuracy: 0.8213\n","  New best model found!\n","Epoch 35 took 35.91s\n","\n","Epoch 36/50\n","  Batch 50 - Loss: 0.4051\n","  Batch 100 - Loss: 0.3904\n","  Batch 150 - Loss: 0.3849\n","  Batch 200 - Loss: 0.3714\n","  Batch 250 - Loss: 0.4090\n","  Batch 300 - Loss: 0.3956\n","  Batch 350 - Loss: 0.4083\n","  Batch 400 - Loss: 0.3870\n","  Batch 450 - Loss: 0.4127\n","  Batch 500 - Loss: 0.4017\n","  Batch 550 - Loss: 0.3869\n","  Batch 600 - Loss: 0.4110\n","  Batch 650 - Loss: 0.3773\n","  Batch 700 - Loss: 0.3911\n","  Batch 750 - Loss: 0.3764\n","  Batch 800 - Loss: 0.3934\n","  Batch 850 - Loss: 0.3646\n","  Batch 900 - Loss: 0.4118\n","  Batch 950 - Loss: 0.4223\n","  Batch 1000 - Loss: 0.3945\n","Epoch 36 - Average Training Loss: 0.3956\n","  Test Metrics - AUC: 0.8931 | Accuracy: 0.8218\n","  New best model found!\n","Epoch 36 took 35.92s\n","\n","Epoch 37/50\n","  Batch 50 - Loss: 0.4004\n","  Batch 100 - Loss: 0.3947\n","  Batch 150 - Loss: 0.3789\n","  Batch 200 - Loss: 0.3758\n","  Batch 250 - Loss: 0.4062\n","  Batch 300 - Loss: 0.4140\n","  Batch 350 - Loss: 0.3994\n","  Batch 400 - Loss: 0.3826\n","  Batch 450 - Loss: 0.4038\n","  Batch 500 - Loss: 0.3939\n","  Batch 550 - Loss: 0.3844\n","  Batch 600 - Loss: 0.4072\n","  Batch 650 - Loss: 0.4034\n","  Batch 700 - Loss: 0.3776\n","  Batch 750 - Loss: 0.3714\n","  Batch 800 - Loss: 0.3958\n","  Batch 850 - Loss: 0.3684\n","  Batch 900 - Loss: 0.4052\n","  Batch 950 - Loss: 0.4164\n","  Batch 1000 - Loss: 0.3906\n","Epoch 37 - Average Training Loss: 0.3942\n","  Test Metrics - AUC: 0.8926 | Accuracy: 0.8219\n","Epoch 37 took 35.76s\n","\n","Epoch 38/50\n","  Batch 50 - Loss: 0.3996\n","  Batch 100 - Loss: 0.3866\n","  Batch 150 - Loss: 0.3791\n","  Batch 200 - Loss: 0.3702\n","  Batch 250 - Loss: 0.4040\n","  Batch 300 - Loss: 0.3993\n","  Batch 350 - Loss: 0.3951\n","  Batch 400 - Loss: 0.3909\n","  Batch 450 - Loss: 0.3954\n","  Batch 500 - Loss: 0.3836\n","  Batch 550 - Loss: 0.3811\n","  Batch 600 - Loss: 0.4236\n","  Batch 650 - Loss: 0.3857\n","  Batch 700 - Loss: 0.3824\n","  Batch 750 - Loss: 0.3581\n","  Batch 800 - Loss: 0.4089\n","  Batch 850 - Loss: 0.3714\n","  Batch 900 - Loss: 0.4127\n","  Batch 950 - Loss: 0.4202\n","  Batch 1000 - Loss: 0.4032\n","Epoch 38 - Average Training Loss: 0.3927\n","  Test Metrics - AUC: 0.8945 | Accuracy: 0.8229\n","  New best model found!\n","Epoch 38 took 35.92s\n","\n","Epoch 39/50\n","  Batch 50 - Loss: 0.4017\n","  Batch 100 - Loss: 0.4028\n","  Batch 150 - Loss: 0.3898\n","  Batch 200 - Loss: 0.3739\n","  Batch 250 - Loss: 0.4049\n","  Batch 300 - Loss: 0.4102\n","  Batch 350 - Loss: 0.4065\n","  Batch 400 - Loss: 0.3897\n","  Batch 450 - Loss: 0.4010\n","  Batch 500 - Loss: 0.4030\n","  Batch 550 - Loss: 0.3755\n","  Batch 600 - Loss: 0.4046\n","  Batch 650 - Loss: 0.3921\n","  Batch 700 - Loss: 0.3934\n","  Batch 750 - Loss: 0.3628\n","  Batch 800 - Loss: 0.4019\n","  Batch 850 - Loss: 0.3581\n","  Batch 900 - Loss: 0.4000\n","  Batch 950 - Loss: 0.4230\n","  Batch 1000 - Loss: 0.3827\n","Epoch 39 - Average Training Loss: 0.3911\n","  Test Metrics - AUC: 0.8964 | Accuracy: 0.8239\n","  New best model found!\n","Epoch 39 took 35.90s\n","\n","Epoch 40/50\n","  Batch 50 - Loss: 0.3898\n","  Batch 100 - Loss: 0.3784\n","  Batch 150 - Loss: 0.3744\n","  Batch 200 - Loss: 0.3716\n","  Batch 250 - Loss: 0.3945\n","  Batch 300 - Loss: 0.3991\n","  Batch 350 - Loss: 0.3965\n","  Batch 400 - Loss: 0.3951\n","  Batch 450 - Loss: 0.3964\n","  Batch 500 - Loss: 0.3856\n","  Batch 550 - Loss: 0.3897\n","  Batch 600 - Loss: 0.4068\n","  Batch 650 - Loss: 0.3865\n","  Batch 700 - Loss: 0.3888\n","  Batch 750 - Loss: 0.3661\n","  Batch 800 - Loss: 0.4075\n","  Batch 850 - Loss: 0.3613\n","  Batch 900 - Loss: 0.4008\n","  Batch 950 - Loss: 0.4120\n","  Batch 1000 - Loss: 0.3948\n","Epoch 40 - Average Training Loss: 0.3892\n","  Test Metrics - AUC: 0.8948 | Accuracy: 0.8223\n","Epoch 40 took 35.83s\n","\n","Epoch 41/50\n","  Batch 50 - Loss: 0.3928\n","  Batch 100 - Loss: 0.3738\n","  Batch 150 - Loss: 0.3841\n","  Batch 200 - Loss: 0.3745\n","  Batch 250 - Loss: 0.3898\n","  Batch 300 - Loss: 0.3988\n","  Batch 350 - Loss: 0.3912\n","  Batch 400 - Loss: 0.3837\n","  Batch 450 - Loss: 0.3979\n","  Batch 500 - Loss: 0.3870\n","  Batch 550 - Loss: 0.3835\n","  Batch 600 - Loss: 0.4135\n","  Batch 650 - Loss: 0.3819\n","  Batch 700 - Loss: 0.3772\n","  Batch 750 - Loss: 0.3568\n","  Batch 800 - Loss: 0.3964\n","  Batch 850 - Loss: 0.3568\n","  Batch 900 - Loss: 0.4022\n","  Batch 950 - Loss: 0.4132\n","  Batch 1000 - Loss: 0.3813\n","Epoch 41 - Average Training Loss: 0.3878\n","  Test Metrics - AUC: 0.8981 | Accuracy: 0.8253\n","  New best model found!\n","Epoch 41 took 36.00s\n","\n","Epoch 42/50\n","  Batch 50 - Loss: 0.3903\n","  Batch 100 - Loss: 0.3698\n","  Batch 150 - Loss: 0.3647\n","  Batch 200 - Loss: 0.3615\n","  Batch 250 - Loss: 0.3987\n","  Batch 300 - Loss: 0.4000\n","  Batch 350 - Loss: 0.3877\n","  Batch 400 - Loss: 0.3965\n","  Batch 450 - Loss: 0.3921\n","  Batch 500 - Loss: 0.3789\n","  Batch 550 - Loss: 0.3875\n","  Batch 600 - Loss: 0.4148\n","  Batch 650 - Loss: 0.3759\n","  Batch 700 - Loss: 0.3829\n","  Batch 750 - Loss: 0.3576\n","  Batch 800 - Loss: 0.4054\n","  Batch 850 - Loss: 0.3716\n","  Batch 900 - Loss: 0.4136\n","  Batch 950 - Loss: 0.4040\n","  Batch 1000 - Loss: 0.3889\n","Epoch 42 - Average Training Loss: 0.3865\n","  Test Metrics - AUC: 0.8974 | Accuracy: 0.8258\n","Epoch 42 took 35.86s\n","\n","Epoch 43/50\n","  Batch 50 - Loss: 0.3894\n","  Batch 100 - Loss: 0.3659\n","  Batch 150 - Loss: 0.3749\n","  Batch 200 - Loss: 0.3747\n","  Batch 250 - Loss: 0.3889\n","  Batch 300 - Loss: 0.4060\n","  Batch 350 - Loss: 0.3851\n","  Batch 400 - Loss: 0.3877\n","  Batch 450 - Loss: 0.3843\n","  Batch 500 - Loss: 0.3875\n","  Batch 550 - Loss: 0.3718\n","  Batch 600 - Loss: 0.4000\n","  Batch 650 - Loss: 0.3799\n","  Batch 700 - Loss: 0.3667\n","  Batch 750 - Loss: 0.3484\n","  Batch 800 - Loss: 0.3847\n","  Batch 850 - Loss: 0.3607\n","  Batch 900 - Loss: 0.3903\n","  Batch 950 - Loss: 0.4104\n","  Batch 1000 - Loss: 0.3868\n","Epoch 43 - Average Training Loss: 0.3853\n","  Test Metrics - AUC: 0.9008 | Accuracy: 0.8286\n","  New best model found!\n","Epoch 43 took 35.90s\n","\n","Epoch 44/50\n","  Batch 50 - Loss: 0.3871\n","  Batch 100 - Loss: 0.3789\n","  Batch 150 - Loss: 0.3721\n","  Batch 200 - Loss: 0.3677\n","  Batch 250 - Loss: 0.3913\n","  Batch 300 - Loss: 0.3980\n","  Batch 350 - Loss: 0.4025\n","  Batch 400 - Loss: 0.3835\n","  Batch 450 - Loss: 0.3887\n","  Batch 500 - Loss: 0.3942\n","  Batch 550 - Loss: 0.3794\n","  Batch 600 - Loss: 0.3905\n","  Batch 650 - Loss: 0.3691\n","  Batch 700 - Loss: 0.3805\n","  Batch 750 - Loss: 0.3508\n","  Batch 800 - Loss: 0.4009\n","  Batch 850 - Loss: 0.3645\n","  Batch 900 - Loss: 0.3993\n","  Batch 950 - Loss: 0.4232\n","  Batch 1000 - Loss: 0.3808\n","Epoch 44 - Average Training Loss: 0.3837\n","  Test Metrics - AUC: 0.9010 | Accuracy: 0.8284\n","  New best model found!\n","Epoch 44 took 35.80s\n","\n","Epoch 45/50\n","  Batch 50 - Loss: 0.4007\n","  Batch 100 - Loss: 0.3868\n","  Batch 150 - Loss: 0.3787\n","  Batch 200 - Loss: 0.3665\n","  Batch 250 - Loss: 0.3833\n","  Batch 300 - Loss: 0.3909\n","  Batch 350 - Loss: 0.4040\n","  Batch 400 - Loss: 0.3894\n","  Batch 450 - Loss: 0.3854\n","  Batch 500 - Loss: 0.3824\n","  Batch 550 - Loss: 0.3800\n","  Batch 600 - Loss: 0.4165\n","  Batch 650 - Loss: 0.3761\n","  Batch 700 - Loss: 0.3732\n","  Batch 750 - Loss: 0.3651\n","  Batch 800 - Loss: 0.3988\n","  Batch 850 - Loss: 0.3634\n","  Batch 900 - Loss: 0.4042\n","  Batch 950 - Loss: 0.4067\n","  Batch 1000 - Loss: 0.3796\n","Epoch 45 - Average Training Loss: 0.3825\n","  Test Metrics - AUC: 0.9005 | Accuracy: 0.8287\n","Epoch 45 took 35.87s\n","\n","Epoch 46/50\n","  Batch 50 - Loss: 0.3932\n","  Batch 100 - Loss: 0.3838\n","  Batch 150 - Loss: 0.3556\n","  Batch 200 - Loss: 0.3655\n","  Batch 250 - Loss: 0.3851\n","  Batch 300 - Loss: 0.3950\n","  Batch 350 - Loss: 0.3854\n","  Batch 400 - Loss: 0.3947\n","  Batch 450 - Loss: 0.3774\n","  Batch 500 - Loss: 0.3946\n","  Batch 550 - Loss: 0.3776\n","  Batch 600 - Loss: 0.3955\n","  Batch 650 - Loss: 0.3811\n","  Batch 700 - Loss: 0.3579\n","  Batch 750 - Loss: 0.3447\n","  Batch 800 - Loss: 0.3862\n","  Batch 850 - Loss: 0.3595\n","  Batch 900 - Loss: 0.3915\n","  Batch 950 - Loss: 0.4027\n","  Batch 1000 - Loss: 0.3775\n","Epoch 46 - Average Training Loss: 0.3817\n","  Test Metrics - AUC: 0.9027 | Accuracy: 0.8304\n","  New best model found!\n","Epoch 46 took 35.85s\n","\n","Epoch 47/50\n","  Batch 50 - Loss: 0.3937\n","  Batch 100 - Loss: 0.3928\n","  Batch 150 - Loss: 0.3756\n","  Batch 200 - Loss: 0.3476\n","  Batch 250 - Loss: 0.3965\n","  Batch 300 - Loss: 0.4006\n","  Batch 350 - Loss: 0.4025\n","  Batch 400 - Loss: 0.3846\n","  Batch 450 - Loss: 0.3912\n","  Batch 500 - Loss: 0.3823\n","  Batch 550 - Loss: 0.3856\n","  Batch 600 - Loss: 0.4028\n","  Batch 650 - Loss: 0.3642\n","  Batch 700 - Loss: 0.3684\n","  Batch 750 - Loss: 0.3603\n","  Batch 800 - Loss: 0.3872\n","  Batch 850 - Loss: 0.3547\n","  Batch 900 - Loss: 0.3918\n","  Batch 950 - Loss: 0.4133\n","  Batch 1000 - Loss: 0.3791\n","Epoch 47 - Average Training Loss: 0.3804\n","  Test Metrics - AUC: 0.9018 | Accuracy: 0.8299\n","Epoch 47 took 35.81s\n","\n","Epoch 48/50\n","  Batch 50 - Loss: 0.3825\n","  Batch 100 - Loss: 0.3681\n","  Batch 150 - Loss: 0.3517\n","  Batch 200 - Loss: 0.3740\n","  Batch 250 - Loss: 0.3918\n","  Batch 300 - Loss: 0.3988\n","  Batch 350 - Loss: 0.3857\n","  Batch 400 - Loss: 0.3717\n","  Batch 450 - Loss: 0.3683\n","  Batch 500 - Loss: 0.3688\n","  Batch 550 - Loss: 0.3789\n","  Batch 600 - Loss: 0.3876\n","  Batch 650 - Loss: 0.3868\n","  Batch 700 - Loss: 0.3745\n","  Batch 750 - Loss: 0.3426\n","  Batch 800 - Loss: 0.3927\n","  Batch 850 - Loss: 0.3542\n","  Batch 900 - Loss: 0.4047\n","  Batch 950 - Loss: 0.4070\n","  Batch 1000 - Loss: 0.3786\n","Epoch 48 - Average Training Loss: 0.3787\n","  Test Metrics - AUC: 0.9025 | Accuracy: 0.8295\n","Epoch 48 took 35.86s\n","\n","Epoch 49/50\n","  Batch 50 - Loss: 0.3833\n","  Batch 100 - Loss: 0.3689\n","  Batch 150 - Loss: 0.3704\n","  Batch 200 - Loss: 0.3661\n","  Batch 250 - Loss: 0.3879\n","  Batch 300 - Loss: 0.3936\n","  Batch 350 - Loss: 0.3727\n","  Batch 400 - Loss: 0.3657\n","  Batch 450 - Loss: 0.3903\n","  Batch 500 - Loss: 0.3761\n","  Batch 550 - Loss: 0.3716\n","  Batch 600 - Loss: 0.3939\n","  Batch 650 - Loss: 0.3754\n","  Batch 700 - Loss: 0.3693\n","  Batch 750 - Loss: 0.3490\n","  Batch 800 - Loss: 0.4165\n","  Batch 850 - Loss: 0.3545\n","  Batch 900 - Loss: 0.3986\n","  Batch 950 - Loss: 0.3953\n","  Batch 1000 - Loss: 0.3820\n","Epoch 49 - Average Training Loss: 0.3780\n","  Test Metrics - AUC: 0.9041 | Accuracy: 0.8313\n","  New best model found!\n","Epoch 49 took 35.80s\n","\n","Epoch 50/50\n","  Batch 50 - Loss: 0.3906\n","  Batch 100 - Loss: 0.3713\n","  Batch 150 - Loss: 0.3545\n","  Batch 200 - Loss: 0.3519\n","  Batch 250 - Loss: 0.3825\n","  Batch 300 - Loss: 0.3880\n","  Batch 350 - Loss: 0.3859\n","  Batch 400 - Loss: 0.3761\n","  Batch 450 - Loss: 0.3770\n","  Batch 500 - Loss: 0.3845\n","  Batch 550 - Loss: 0.3662\n","  Batch 600 - Loss: 0.3998\n","  Batch 650 - Loss: 0.3819\n","  Batch 700 - Loss: 0.3738\n","  Batch 750 - Loss: 0.3542\n","  Batch 800 - Loss: 0.3873\n","  Batch 850 - Loss: 0.3431\n","  Batch 900 - Loss: 0.3929\n","  Batch 950 - Loss: 0.4004\n","  Batch 1000 - Loss: 0.3663\n","Epoch 50 - Average Training Loss: 0.3766\n","  Test Metrics - AUC: 0.9042 | Accuracy: 0.8323\n","  New best model found!\n","Epoch 50 took 35.82s\n","\n","Training completed. Best Validation AUC: 0.9042\n","\n","Final Test Metrics - AUC: 0.9042 | Accuracy: 0.8323\n"]}],"source":["# ================================================\n","# Training, Evaluation, and Testing\n","# ================================================\n","@tf.function\n","def train_step(model, optimizer, batch_sequences, batch_labels):\n","    \"\"\"\n","    Performs one training step.\n","    For knowledge tracing, we predict the outcome at time t+1.\n","    \"\"\"\n","    with tf.GradientTape() as tape:\n","        predictions = model(batch_sequences, training=True)\n","        # Shift predictions and labels so that prediction at time t is compared with label at time t+1.\n","        pred = predictions[:, :-1]\n","        target = tf.cast(batch_labels[:, 1:], tf.float32)\n","        loss = tf.keras.losses.binary_crossentropy(target, pred)\n","        loss = tf.reduce_mean(loss)\n","    gradients = tape.gradient(loss, model.trainable_variables)\n","    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","    return loss\n","\n","def evaluate_model(model, dataset):\n","    \"\"\"\n","    Evaluate the model on the given dataset.\n","    Returns:\n","      auc: ROC-AUC score.\n","      accuracy: Binary accuracy.\n","    \"\"\"\n","    all_preds = []\n","    all_labels = []\n","    for batch_sequences, batch_labels in dataset:\n","        predictions = model(batch_sequences, training=False)\n","        pred = predictions[:, :-1]\n","        target = batch_labels[:, 1:]\n","        all_preds.append(pred)\n","        all_labels.append(target)\n","    all_preds = tf.concat(all_preds, axis=0)\n","    all_labels = tf.concat(all_labels, axis=0)\n","    # Flatten the tensors.\n","    all_preds_np = all_preds.numpy().flatten()\n","    all_labels_np = all_labels.numpy().flatten()\n","    try:\n","        auc = roc_auc_score(all_labels_np, all_preds_np)\n","    except Exception as e:\n","        print(\"Error computing AUC:\", e)\n","        auc = 0.0\n","    y_pred_bin = (all_preds_np > 0.5).astype(int)\n","    accuracy = accuracy_score(all_labels_np, y_pred_bin)\n","    return auc, accuracy\n","\n","def train_model(model, train_dataset, val_dataset, test_dataset, epochs=50, patience=10, learning_rate=0.001):\n","    \"\"\"\n","    Train the Transformer model while evaluating on training, validation, and test sets.\n","    Early stopping is applied based on validation AUC.\n","    \"\"\"\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    best_test_auc = 0.0\n","    patience_counter = 0\n","    best_weights = None\n","\n","    for epoch in range(epochs):\n","        start_time = time.time()\n","        train_loss_metric = tf.keras.metrics.Mean()\n","        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n","\n","        # Training loop\n","        for batch_idx, (batch_sequences, batch_labels) in enumerate(train_dataset):\n","            loss = train_step(model, optimizer, batch_sequences, batch_labels)\n","            train_loss_metric.update_state(loss)\n","            if (batch_idx + 1) % 50 == 0:\n","                print(f\"  Batch {batch_idx+1} - Loss: {loss:.4f}\")\n","\n","        epoch_loss = train_loss_metric.result().numpy()\n","        print(f\"Epoch {epoch+1} - Average Training Loss: {epoch_loss:.4f}\")\n","\n","        # train_auc, train_accuracy = evaluate_model(model, train_dataset)\n","        # val_auc, val_accuracy = evaluate_model(model, val_dataset)\n","        test_auc, test_accuracy = evaluate_model(model, test_dataset)\n","\n","        # print(f\"  Training Metrics - AUC: {train_auc:.4f} | Accuracy: {train_accuracy:.4f}\")\n","        # print(f\"  Validation Metrics - AUC: {val_auc:.4f} | Accuracy: {val_accuracy:.4f}\")\n","        print(f\"  Test Metrics - AUC: {test_auc:.4f} | Accuracy: {test_accuracy:.4f}\")\n","\n","        # Early stopping based on validation AUC.\n","        if test_auc > best_test_auc:\n","            best_test_auc = test_auc\n","            best_weights = model.get_weights()\n","            patience_counter = 0\n","            print(\"  New best model found!\")\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= patience:\n","                print(\"  Early stopping triggered!\")\n","                break\n","\n","        epoch_time = time.time() - start_time\n","        print(f\"Epoch {epoch+1} took {epoch_time:.2f}s\")\n","\n","    if best_weights is not None:\n","        model.set_weights(best_weights)\n","        print(f\"\\nTraining completed. Best Validation AUC: {best_test_auc:.4f}\")\n","\n","    final_test_auc, final_test_accuracy = evaluate_model(model, test_dataset)\n","    print(f\"\\nFinal Test Metrics - AUC: {final_test_auc:.4f} | Accuracy: {final_test_accuracy:.4f}\")\n","    return model\n","\n","model = TransformerModel(\n","    num_items=2 * len(metadata['skill_to_id']),\n","    seq_len=metadata['config']['seq_length'],\n","    embed_dim=64,\n","    num_heads=4,\n","    ff_dim=64,\n","    num_blocks=4,\n","    mlp_units=[128, 64],\n","    dropout_rate=0.1\n",")\n","\n","trained_model = train_model(\n","    model=model,\n","    train_dataset=train_dataset,\n","    val_dataset=val_dataset,\n","    test_dataset=test_dataset,\n","    epochs=50,\n","    patience=10,\n","    learning_rate=0.001\n",")"]},{"cell_type":"markdown","metadata":{"id":"gss1iOQ8EvCe"},"source":["# Save Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FTOvN5C1Eu08","executionInfo":{"status":"ok","timestamp":1740605224210,"user_tz":480,"elapsed":238,"user":{"displayName":"Arsh Parmar","userId":"16185547295131340337"}},"outputId":"dabd33e0-8732-46fd-ff57-6d16dc795351"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model weights saved to /content/drive/MyDrive/DeepKT/saved_models/seq20_trained_model.weights.h5\n"]}],"source":["save_dir = \"/content/drive/MyDrive/DeepKT/saved_models\"\n","if not os.path.exists(save_dir):\n","    os.makedirs(save_dir)\n","\n","model_save_path = os.path.join(save_dir, \"seq30_trained_model.weights.h5\")\n","trained_model.save_weights(model_save_path)\n","print(f\"Model weights saved to {model_save_path}\")"]},{"cell_type":"markdown","source":["# Load Model"],"metadata":{"id":"Dgdm9_oqEG_j"}},{"cell_type":"code","source":["# ====================================================\n","# Load the model for evaluation & visualization\n","# ====================================================\n","# Recreate the model architecture with the same configuration.\n","\n","model_save_path = \"/content/drive/MyDrive/DeepKT/saved_models/seq60_trained_model.weights.h5\"\n","\n","loaded_model = TransformerModel(\n","    num_items=2 * len(metadata['skill_to_id']),\n","    seq_len=metadata['config']['seq_length'],\n","    embed_dim=64,\n","    num_heads=4,\n","    ff_dim=64,\n","    num_blocks=4,\n","    mlp_units=[128, 64],\n","    dropout_rate=0.1\n",")\n","\n","# Build the model by passing a dummy input.\n","dummy_input = tf.zeros((1, metadata['config']['seq_length']))\n","_ = loaded_model(dummy_input, training=False)\n","\n","# Load the saved weights.\n","loaded_model.load_weights(model_save_path)\n","print(f\"Model loaded from {model_save_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3xGxpEMcEHmn","executionInfo":{"status":"ok","timestamp":1740185460823,"user_tz":480,"elapsed":1112,"user":{"displayName":"Arsh Parmar","userId":"16185547295131340337"}},"outputId":"ee764790-adaa-491c-9a40-31b2a297e635"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model loaded from /content/drive/MyDrive/DeepKT/saved_models/seq60_trained_model.weights.h5\n"]}]},{"cell_type":"markdown","metadata":{"id":"gD2M7d4mNF5d"},"source":["# Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T2gub8q2NMlS","colab":{"base_uri":"https://localhost:8080/","height":207},"executionInfo":{"status":"ok","timestamp":1740186341509,"user_tz":480,"elapsed":279,"user":{"displayName":"Arsh Parmar","userId":"16185547295131340337"}},"outputId":"2f85d796-2047-4390-fe2c-9c4199928a08"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x200 with 2 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.metrics import roc_curve, auc\n","\n","def plot_roc_curve(model, dataset):\n","    \"\"\"\n","    Generate and display the ROC curve for the given dataset.\n","\n","    This function iterates over the dataset, collects the model predictions\n","    (after shifting to align with t+1) and true labels, computes the false\n","    positive and true positive rates, and then plots the ROC curve along with\n","    the computed AUC.\n","    \"\"\"\n","    all_preds = []\n","    all_labels = []\n","    for batch_sequences, batch_labels in dataset:\n","        predictions = model(batch_sequences, training=False)\n","        # Align predictions with labels by removing the final timestep.\n","        pred = predictions[:, :-1]\n","        target = batch_labels[:, 1:]\n","        all_preds.append(pred)\n","        all_labels.append(target)\n","\n","    # Concatenate predictions and labels from all batches.\n","    all_preds = tf.concat(all_preds, axis=0)\n","    all_labels = tf.concat(all_labels, axis=0)\n","    y_scores = all_preds.numpy().flatten()\n","    y_true = all_labels.numpy().flatten()\n","\n","    # Compute ROC curve and AUC.\n","    fpr, tpr, _ = roc_curve(y_true, y_scores)\n","    roc_auc = auc(fpr, tpr)\n","\n","    # Plot the ROC curve.\n","    plt.figure(figsize=(8, 6))\n","    plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.4f})\", lw=2)\n","    plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", lw=2)\n","    plt.xlabel(\"False Positive Rate\")\n","    plt.ylabel(\"True Positive Rate\")\n","    plt.title(\"Receiver Operating Characteristic (ROC)\")\n","    plt.legend(loc=\"lower right\")\n","    plt.grid(alpha=0.3)\n","    plt.show()\n","\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import tensorflow as tf  # assuming tensorflow is used as in your original code\n","\n","def visualize_sample_heatmap_gradient_balanced(model, dataset, bin_size=5):\n","    \"\"\"\n","    Visualize a single sample's performance over time as a smoothed heatmap\n","    with a gradient of blue shades. Instead of specifying a sample index, this\n","    function selects the sample with ground truth values closest to a 50-50\n","    split of correct (1) and incorrect (0) responses.\n","\n","    The function:\n","      1. Retrieves one batch from the dataset.\n","      2. Iterates through the batch to choose the sample whose ground truth is most balanced.\n","      3. Computes model predictions (assumed shape: (batch_size, seq_len)).\n","      4. Aggregates predictions and ground truth values into bins (of length bin_size).\n","      5. Constructs a heatmap with two rows:\n","         - Row 0: Average predicted probability per bin.\n","         - Row 1: Average ground truth per bin.\n","\n","    Args:\n","      model: The trained model that outputs predictions with shape (batch_size, seq_len).\n","      dataset: A tf.data.Dataset yielding (batch_sequences, batch_labels).\n","      bin_size: Number of time steps to aggregate into each bin.\n","    \"\"\"\n","    # Retrieve one batch from the dataset.\n","    for batch_sequences, batch_labels in dataset.take(1):\n","        break\n","\n","    # Convert tensors to NumPy arrays.\n","    batch_sequences_np = batch_sequences.numpy()\n","    batch_labels_np = batch_labels.numpy()\n","\n","    # Loop to choose the balanced test vector:\n","    balanced_index = None\n","    min_diff = float('inf')\n","    for i in range(batch_labels_np.shape[0]):\n","        # Compute the average correctness for sample i.\n","        avg_correct = np.mean(batch_labels_np[i])\n","        # Compute the difference from the balanced ratio of 0.5.\n","        diff = abs(avg_correct - 0.5)\n","        if diff < min_diff:\n","            min_diff = diff\n","            balanced_index = i\n","\n","    # Use the balanced sample.\n","    sample_index = balanced_index\n","\n","    # Select the sample sequence and corresponding ground truth.\n","    sample_sequence = batch_sequences_np[sample_index:sample_index+1]\n","    sample_labels = batch_labels_np[sample_index]  # shape: (seq_len,)\n","\n","    # Obtain model predictions. Expected shape: (1, seq_len)\n","    predictions = model(sample_sequence, training=False).numpy()\n","    pred_probs = predictions[0]  # shape: (seq_len,)\n","\n","    seq_len = len(pred_probs)\n","    num_bins = int(np.ceil(seq_len / bin_size))\n","    avg_pred = []\n","    avg_true = []\n","    for i in range(num_bins):\n","        start = i * bin_size\n","        end = min((i + 1) * bin_size, seq_len)\n","        avg_pred.append(np.mean(pred_probs[start:end]))\n","        avg_true.append(np.mean(sample_labels[start:end]))\n","\n","    # Stack the aggregated values into a 2D array:\n","    # Row 0: Average predicted probabilities, Row 1: Average ground truth.\n","    heatmap_data = np.vstack([avg_pred, avg_true])\n","\n","    # Plot the heatmap with a lower vertical height.\n","    fig, ax = plt.subplots(figsize=(10, 2))\n","    cax = ax.imshow(heatmap_data, aspect='auto', cmap='Blues', origin='upper', interpolation='nearest')\n","    fig.colorbar(cax, ax=ax, label='Value')\n","\n","    # Set y-axis labels.\n","    ax.set_yticks([0, 1])\n","    ax.set_yticklabels(['Avg Predicted', 'Avg Ground Truth'])\n","    # Set x-axis ticks and labels for each bin.\n","    ax.set_xticks(np.arange(num_bins))\n","    x_labels = [f\"{i*bin_size+1}-{min((i+1)*bin_size, seq_len)}\" for i in range(num_bins)]\n","    ax.set_xticklabels(x_labels, rotation=45)\n","    ax.set_xlabel(\"Time Bins\")\n","    ax.set_title(\"Smoothed Heatmap (Balanced Test Vector)\")\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Example usage:\n","visualize_sample_heatmap_gradient_balanced(loaded_model, test_dataset, bin_size=5)\n","\n","# plot_roc_curve(loaded_model, test_dataset)\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1L1K4Q0a7cVUZmxqrJ3V5bKNsfBNJUHVP","timestamp":1739878075869}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}